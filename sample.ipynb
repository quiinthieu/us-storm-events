{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Case Study: Analyzing U.S. Storm Events for Disaster Preparedness \n",
    "\n",
    "In this case study, we will analyze storm event data to enhance disaster preparedness efforts in the U.S. We will follow a structured data analysis workflow comprising the following phases:\n",
    "\n",
    "1. **Ask**: Define the problem and confirm expectations.\n",
    "2. **Prepare**: Collect and store data for analysis.\n",
    "3. **Process**: Clean and transform data to ensure integrity.\n",
    "4. **Analyze**: Use data analysis tools to draw conclusions.\n",
    "5. **Share**: Interpret and communicate results to make data-driven decisions.\n",
    "6. **Act**: Put insights to work to address the original problem.\n",
    "\n",
    "We will utilize the open-source Storm Event Database provided by the National Oceanic and Atmospheric Administration (NOAA), available at [NOAA Storm Events Database](https://www.ncdc.noaa.gov/stormevents/ftp.jsp). This dataset contains comprehensive information about various storm events across the U.S., including details such as event type, location, and impact."
   ],
   "id": "be7169fdcf7f38c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "45d83b2e739d08a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ask\n",
    "\n",
    "### Problem Statement\n",
    "The U.S. experiences various storm events that can impact public safety, property, and infrastructure. Analyzing the patterns and impacts of these storms will provide insights that can inform disaster preparedness and response strategies.\n",
    "\n",
    "### Key Questions\n",
    "1. **What types of storm events are most prevalent in the U.S.?**  \n",
    "   Identify the most common storm events and their geographical distribution.\n",
    "\n",
    "2. **Where do storm events most frequently occur?**  \n",
    "   Map the geographical distribution of storm events to identify high-risk areas.\n",
    "\n",
    "3. **Are certain regions more prone to specific types of storm events?**  \n",
    "   Analyze the geographic hot spots for storm types to develop tailored regional preparedness plans.\n",
    "\n",
    "4. **What is the temporal distribution of storm events?**  \n",
    "   Analyze trends over time to understand seasonal patterns and changes in frequency.\n",
    "\n",
    "5. **What is the impact of different storm events on injuries, fatalities, and property damage?**  \n",
    "   Assess the severity of various storm types and their associated risks to prioritize high-risk events for preparedness planning.\n",
    "\n",
    "6. **How can this analysis inform disaster preparedness initiatives?**  \n",
    "   Determine actionable insights that can help stakeholders enhance response plans and allocate resources effectively."
   ],
   "id": "2ccab1df027afd0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "7e4d636d4bd247aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare\n",
    "\n",
    "### Data Sources\n",
    "We will use the Storm Events Database from the NOAA for the years 2014 to 2024. The dataset can be accessed at [NOAA Storm Events Database](https://www.ncdc.noaa.gov/stormevents/ftp.jsp).\n",
    "\n",
    "### Data Collection\n",
    "The relevant files will be downloaded in CSV.GZ format, which is a compressed version of CSV files.\n",
    "\n",
    "### Data Description\n",
    "The dataset contains 51 columns with various types of information related to storm events. Below is the data dictionary presented in a table format:\n",
    "\n",
    "\n",
    "| Column Name         | Example                                                    | Description                                                                                                             |\n",
    "|:--------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|\n",
    "| begin_yearmonth     | 201212                                                     | The year and month that the event began (%Y%m format).                                                                  |\n",
    "| begin_day           | 1, 31                                                      | The day of the month that the event began (%-d format).                                                                 |\n",
    "| begin_time          | 800, 2359                                                  | The time of day that the event began (%-H%M format).                                                                     |\n",
    "| end_yearmonth       | 201301                                                     | The year and month that the event ended (%Y%m format).                                                                |\n",
    "| end_day             | 01                                                         | The day of the month that the event ended (%-d format).                                                                  |\n",
    "| end_time            | 0001                                                       | The time of day that the event ended (%-H%M format).                                                                     |\n",
    "| episode_id          | 61280                                                      | ID assigned by NWS to denote the storm episode; may contain multiple events.                                            |\n",
    "| event_id            | 383097                                                     | ID assigned by NWS for each individual storm event (Primary database key field).                                        |\n",
    "| state               | GEORGIA                                                    | The state name where the event occurred (ALL CAPS).                                                                     |\n",
    "| state_fips          | 45                                                         | Unique number assigned to the county by NIST (State FIPS).                                                              |\n",
    "| year                | 2000                                                       | The four-digit year for the event in this record.                                                                       |\n",
    "| month_name          | January                                                    | The name of the month for the event (not abbreviated).                                                                  |\n",
    "| event_type          | Hail                                                       | The type of storm event (spelled out; not abbreviated).                                                                 |\n",
    "| cz_type             | C                                                          | Indicates whether the event happened in a County/Parish, NWS Public Forecast Zone, or Marine.                           |\n",
    "| cz_fips             | 245                                                        | The county FIPS number assigned by NIST or NWS Forecast Zone Number.                                                    |\n",
    "| cz_name             | AIKEN                                                      | Name assigned to the county FIPS number or NWS Forecast Zone.                                                           |\n",
    "| wfo                 | CAE                                                        | The NWS Forecast Officeâ€™s area of responsibility in which the event occurred.                                           |\n",
    "| begin_date_time     | 04/1/2012 20:48:00                                         | Start date and time of the event (MM/DD/YYYY hh:mm:ss).                                                                 |\n",
    "| cz_timezone         | EST-5                                                      | Time Zone for the County/Parish, Zone or Marine Name.                                                                   |\n",
    "| end_date_time       | 04/1/2012 21:03:00                                         | End date and time of the event (MM/DD/YYYY hh:mm:ss).                                                                   |\n",
    "| injuries_direct     | 1                                                          | Number of injuries directly caused by the weather event.                                                                |\n",
    "| injuries_indirect   | 0                                                          | Number of injuries indirectly caused by the weather event.                                                              |\n",
    "| deaths_direct       | 0                                                          | Number of deaths directly caused by the weather event.                                                                  |\n",
    "| deaths_indirect     | 0                                                          | Number of deaths indirectly caused by the weather event.                                                                |\n",
    "| damage_property     | 10.00K                                                     | Estimated property damage incurred by the weather event.                                                                |\n",
    "| damage_crops        | 0.00K                                                      | Estimated damage to crops incurred by the weather event.                                                                |\n",
    "| source              | Public                                                     | Source reporting the weather event.                                                                                     |\n",
    "| magnitude           | 0.75                                                       | Measured extent of the magnitude type (only for wind speeds and hail size).                                             |\n",
    "| magnitude_type      | EG                                                         | Type of magnitude measurement (e.g., wind estimated gust).                                                              |\n",
    "| flood_cause         | Ice Jam                                                    | Reported cause of the flood.                                                                                            |\n",
    "| category            |                                                            | Unknown (During the time of downloading this particular file, NCDC has never seen anything provided within this field.) |\n",
    "| tor_f_scale         | EF0                                                        | Enhanced Fujita Scale describing tornado strength.                                                                      |\n",
    "| tor_length          | 0.66                                                       | Length of the tornado while on the ground (in miles).                                                                   |\n",
    "| tor_width           | 25                                                         | Width of the tornado while on the ground (in whole yards).                                                              |\n",
    "| tor_other_wfo       | DDC                                                        | Continuation of a tornado segment as it crossed from one NWS Forecast Office to another.                                |\n",
    "| tor_other_cz_state  | KS                                                         | Two-character representation for the state name of the continuing tornado segment.                                      |\n",
    "| tor_other_cz_fips   | 41                                                         | FIPS number of the county for the continuing tornado segment.                                                           |\n",
    "| tor_other_cz_name   | DICKINSON                                                  | Name of the county for the continuing tornado segment.                                                                  |\n",
    "| begin_range         | 0.59                                                       | Distance to the nearest tenth of a mile to the location referenced.                                                     |\n",
    "| begin_azimuth       | ENE                                                        | 16-point compass direction from the location referenced.                                                                |\n",
    "| begin_location      | PINELAND                                                   | Name of city, town, or village from which the range is calculated.                                                      |\n",
    "| end_range           | 0.66                                                       | See begin_range.                                                                                                        |\n",
    "| end_azimuth         | WNW                                                        | See begin_azimuth.                                                                                                      |\n",
    "| end_location        | RUSK                                                       | See begin_location.                                                                                                     |\n",
    "| begin_lat           | 29.7898                                                    | Latitude in decimal degrees of the begin point of the event.                                                            |\n",
    "| begin_lon           | -98.6406                                                   | Longitude in decimal degrees of the begin point of the event.                                                           |\n",
    "| end_lat             | 29.7158                                                    | Latitude in decimal degrees of the end point of the event.                                                              |\n",
    "| end_lon             | -98.7744                                                   | Longitude in decimal degrees of the end point of the event.                                                             |\n",
    "| episode_narrative   | A strong upper level system over the southern Rockies...   | Narrative depicting the general nature of the episode.                                                                  |\n",
    "| event_narrative     | Heavy rain caused flash flooding across parts of Wilber... | Narrative providing descriptive details of the individual event.                                                        |\n",
    "\n",
    "### Data Storage\n",
    "The data will be stored in a local environment for analysis."
   ],
   "id": "7d62c9500d2cc5bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:28:49.567713Z",
     "start_time": "2024-09-24T11:28:49.561520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To begin, let's import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import glob\n",
    "import missingno as msno\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Set display options to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "id": "154448bb887011bb",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:28:56.713694Z",
     "start_time": "2024-09-24T11:28:52.057658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In the data folder, we have lots of files. But for this analysis, we only need Storm Details files which have the prefix \"StormEvents_details-ftp_v1.0_d\".\n",
    "# So, let's define the file path pattern\n",
    "file_pattern = \"./data/StormEvents_details-ftp_v1.0_d*.csv.gz\"\n",
    "\n",
    "# Get a list of all files matching the pattern\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "# Read and concatenate all files into a single DataFrame\n",
    "df_list = [pd.read_csv(file, compression='gzip') for file in all_files]\n",
    "df_details = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Rename column names to snake case for consistency \n",
    "df_details = df_details.clean_names()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_details.head()"
   ],
   "id": "a8e23b6fa0536bca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   begin_yearmonth  begin_day  begin_time  end_yearmonth  end_day  end_time  \\\n",
       "0           202112         11         349         202112       11       350   \n",
       "1           202112         11         249         202112       11       254   \n",
       "2           202112         11         325         202112       11       327   \n",
       "3           202112         11         232         202112       11       239   \n",
       "4           202112          6         724         202112        6       724   \n",
       "\n",
       "   episode_id  event_id      state  state_fips  year month_name  \\\n",
       "0      165322    999750  TENNESSEE          47  2021   December   \n",
       "1      165322    999613  TENNESSEE          47  2021   December   \n",
       "2      165322    999636  TENNESSEE          47  2021   December   \n",
       "3      165322    999604  TENNESSEE          47  2021   December   \n",
       "4      165321    999306  TENNESSEE          47  2021   December   \n",
       "\n",
       "          event_type cz_type  cz_fips   cz_name  wfo     begin_date_time  \\\n",
       "0            Tornado       C      165    SUMNER  OHX  11-DEC-21 03:49:00   \n",
       "1            Tornado       C       43   DICKSON  OHX  11-DEC-21 02:49:00   \n",
       "2  Thunderstorm Wind       C       37  DAVIDSON  OHX  11-DEC-21 03:25:00   \n",
       "3            Tornado       C       81   HICKMAN  OHX  11-DEC-21 02:32:00   \n",
       "4  Thunderstorm Wind       C       49  FENTRESS  OHX  06-DEC-21 07:24:00   \n",
       "\n",
       "  cz_timezone       end_date_time  injuries_direct  injuries_indirect  \\\n",
       "0       CST-6  11-DEC-21 03:50:00                0                  0   \n",
       "1       CST-6  11-DEC-21 02:54:00                0                  0   \n",
       "2       CST-6  11-DEC-21 03:27:00                0                  0   \n",
       "3       CST-6  11-DEC-21 02:39:00                0                  0   \n",
       "4       CST-6  06-DEC-21 07:24:00                0                  0   \n",
       "\n",
       "   deaths_direct  deaths_indirect damage_property damage_crops  \\\n",
       "0              0                0          10.00K        0.00K   \n",
       "1              0                0          10.00K        0.00K   \n",
       "2              0                0         250.00K        0.00K   \n",
       "3              0                0          50.00K        0.00K   \n",
       "4              0                0           3.00K        0.00K   \n",
       "\n",
       "             source  magnitude magnitude_type flood_cause  category  \\\n",
       "0  NWS Storm Survey        NaN            NaN         NaN       NaN   \n",
       "1  NWS Storm Survey        NaN            NaN         NaN       NaN   \n",
       "2  NWS Storm Survey       74.0             EG         NaN       NaN   \n",
       "3  NWS Storm Survey        NaN            NaN         NaN       NaN   \n",
       "4      Social Media       52.0             EG         NaN       NaN   \n",
       "\n",
       "  tor_f_scale  tor_length  tor_width tor_other_wfo tor_other_cz_state  \\\n",
       "0         EF0        1.72       50.0           OHX                 TN   \n",
       "1         EF0        5.41      175.0           NaN                NaN   \n",
       "2         NaN         NaN        NaN           NaN                NaN   \n",
       "3         EF1        8.54      400.0           NaN                NaN   \n",
       "4         NaN         NaN        NaN           NaN                NaN   \n",
       "\n",
       "   tor_other_cz_fips tor_other_cz_name  begin_range begin_azimuth  \\\n",
       "0              189.0            WILSON          3.0           WNW   \n",
       "1                NaN               NaN          1.0           ENE   \n",
       "2                NaN               NaN          1.0            NW   \n",
       "3                NaN               NaN          4.0            NW   \n",
       "4                NaN               NaN          1.0             W   \n",
       "\n",
       "  begin_location  end_range end_azimuth end_location  begin_lat  begin_lon  \\\n",
       "0     HUNTERS PT        3.0          NW   HUNTERS PT    36.3178   -86.3235   \n",
       "1        TIDWELL        2.0         ESE  BAKERSWORKS    36.0255   -87.3054   \n",
       "2      MAPLEWOOD        2.0          SW        AMQUI    36.2372   -86.7286   \n",
       "3           SPOT        4.0         NNW     PINEWOOD    35.9205   -87.6423   \n",
       "4      JAMESTOWN        1.0           W    JAMESTOWN    36.4322   -84.9405   \n",
       "\n",
       "   end_lat  end_lon                                  episode_narrative  \\\n",
       "0  36.3296 -86.2965  One of the worst tornado outbreaks ever record...   \n",
       "1  36.0736 -87.2330  One of the worst tornado outbreaks ever record...   \n",
       "2  36.2572 -86.7035  One of the worst tornado outbreaks ever record...   \n",
       "3  35.9725 -87.5068  One of the worst tornado outbreaks ever record...   \n",
       "4  36.4322 -84.9405  After some isolated thunderstorms moved across...   \n",
       "\n",
       "                                     event_narrative data_source  \n",
       "0  This small EF-0 tornado was determined through...         CSV  \n",
       "1  This tornado developed just southeast of the D...         CSV  \n",
       "2  Severe straight-line winds caused significant ...         CSV  \n",
       "3  This tornado touched down in far northwest Hic...         CSV  \n",
       "4  A Facebook report indicated trees and power li...         CSV  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin_yearmonth</th>\n",
       "      <th>begin_day</th>\n",
       "      <th>begin_time</th>\n",
       "      <th>end_yearmonth</th>\n",
       "      <th>end_day</th>\n",
       "      <th>end_time</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>state</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>year</th>\n",
       "      <th>month_name</th>\n",
       "      <th>event_type</th>\n",
       "      <th>cz_type</th>\n",
       "      <th>cz_fips</th>\n",
       "      <th>cz_name</th>\n",
       "      <th>wfo</th>\n",
       "      <th>begin_date_time</th>\n",
       "      <th>cz_timezone</th>\n",
       "      <th>end_date_time</th>\n",
       "      <th>injuries_direct</th>\n",
       "      <th>injuries_indirect</th>\n",
       "      <th>deaths_direct</th>\n",
       "      <th>deaths_indirect</th>\n",
       "      <th>damage_property</th>\n",
       "      <th>damage_crops</th>\n",
       "      <th>source</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>magnitude_type</th>\n",
       "      <th>flood_cause</th>\n",
       "      <th>category</th>\n",
       "      <th>tor_f_scale</th>\n",
       "      <th>tor_length</th>\n",
       "      <th>tor_width</th>\n",
       "      <th>tor_other_wfo</th>\n",
       "      <th>tor_other_cz_state</th>\n",
       "      <th>tor_other_cz_fips</th>\n",
       "      <th>tor_other_cz_name</th>\n",
       "      <th>begin_range</th>\n",
       "      <th>begin_azimuth</th>\n",
       "      <th>begin_location</th>\n",
       "      <th>end_range</th>\n",
       "      <th>end_azimuth</th>\n",
       "      <th>end_location</th>\n",
       "      <th>begin_lat</th>\n",
       "      <th>begin_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>episode_narrative</th>\n",
       "      <th>event_narrative</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>349</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>350</td>\n",
       "      <td>165322</td>\n",
       "      <td>999750</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>165</td>\n",
       "      <td>SUMNER</td>\n",
       "      <td>OHX</td>\n",
       "      <td>11-DEC-21 03:49:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>11-DEC-21 03:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>NWS Storm Survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EF0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>50.0</td>\n",
       "      <td>OHX</td>\n",
       "      <td>TN</td>\n",
       "      <td>189.0</td>\n",
       "      <td>WILSON</td>\n",
       "      <td>3.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>HUNTERS PT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>HUNTERS PT</td>\n",
       "      <td>36.3178</td>\n",
       "      <td>-86.3235</td>\n",
       "      <td>36.3296</td>\n",
       "      <td>-86.2965</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>This small EF-0 tornado was determined through...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>249</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>254</td>\n",
       "      <td>165322</td>\n",
       "      <td>999613</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>43</td>\n",
       "      <td>DICKSON</td>\n",
       "      <td>OHX</td>\n",
       "      <td>11-DEC-21 02:49:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>11-DEC-21 02:54:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>NWS Storm Survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EF0</td>\n",
       "      <td>5.41</td>\n",
       "      <td>175.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>TIDWELL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>BAKERSWORKS</td>\n",
       "      <td>36.0255</td>\n",
       "      <td>-87.3054</td>\n",
       "      <td>36.0736</td>\n",
       "      <td>-87.2330</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>This tornado developed just southeast of the D...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>325</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>327</td>\n",
       "      <td>165322</td>\n",
       "      <td>999636</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>C</td>\n",
       "      <td>37</td>\n",
       "      <td>DAVIDSON</td>\n",
       "      <td>OHX</td>\n",
       "      <td>11-DEC-21 03:25:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>11-DEC-21 03:27:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>NWS Storm Survey</td>\n",
       "      <td>74.0</td>\n",
       "      <td>EG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>MAPLEWOOD</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>AMQUI</td>\n",
       "      <td>36.2372</td>\n",
       "      <td>-86.7286</td>\n",
       "      <td>36.2572</td>\n",
       "      <td>-86.7035</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>Severe straight-line winds caused significant ...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>232</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>239</td>\n",
       "      <td>165322</td>\n",
       "      <td>999604</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>81</td>\n",
       "      <td>HICKMAN</td>\n",
       "      <td>OHX</td>\n",
       "      <td>11-DEC-21 02:32:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>11-DEC-21 02:39:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>NWS Storm Survey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EF1</td>\n",
       "      <td>8.54</td>\n",
       "      <td>400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>SPOT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>PINEWOOD</td>\n",
       "      <td>35.9205</td>\n",
       "      <td>-87.6423</td>\n",
       "      <td>35.9725</td>\n",
       "      <td>-87.5068</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>This tornado touched down in far northwest Hic...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202112</td>\n",
       "      <td>6</td>\n",
       "      <td>724</td>\n",
       "      <td>202112</td>\n",
       "      <td>6</td>\n",
       "      <td>724</td>\n",
       "      <td>165321</td>\n",
       "      <td>999306</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>C</td>\n",
       "      <td>49</td>\n",
       "      <td>FENTRESS</td>\n",
       "      <td>OHX</td>\n",
       "      <td>06-DEC-21 07:24:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>06-DEC-21 07:24:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>Social Media</td>\n",
       "      <td>52.0</td>\n",
       "      <td>EG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>W</td>\n",
       "      <td>JAMESTOWN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>W</td>\n",
       "      <td>JAMESTOWN</td>\n",
       "      <td>36.4322</td>\n",
       "      <td>-84.9405</td>\n",
       "      <td>36.4322</td>\n",
       "      <td>-84.9405</td>\n",
       "      <td>After some isolated thunderstorms moved across...</td>\n",
       "      <td>A Facebook report indicated trees and power li...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "ba22c65876c9777"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Process",
   "id": "cd0e4cdde66c313e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:28:58.530473Z",
     "start_time": "2024-09-24T11:28:58.235241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the DataFrame information\n",
    "df_details.info()"
   ],
   "id": "b05affd339697305",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 669746 entries, 0 to 669745\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   begin_yearmonth     669746 non-null  int64  \n",
      " 1   begin_day           669746 non-null  int64  \n",
      " 2   begin_time          669746 non-null  int64  \n",
      " 3   end_yearmonth       669746 non-null  int64  \n",
      " 4   end_day             669746 non-null  int64  \n",
      " 5   end_time            669746 non-null  int64  \n",
      " 6   episode_id          669746 non-null  int64  \n",
      " 7   event_id            669746 non-null  int64  \n",
      " 8   state               669746 non-null  object \n",
      " 9   state_fips          669746 non-null  int64  \n",
      " 10  year                669746 non-null  int64  \n",
      " 11  month_name          669746 non-null  object \n",
      " 12  event_type          669746 non-null  object \n",
      " 13  cz_type             669746 non-null  object \n",
      " 14  cz_fips             669746 non-null  int64  \n",
      " 15  cz_name             669746 non-null  object \n",
      " 16  wfo                 669746 non-null  object \n",
      " 17  begin_date_time     669746 non-null  object \n",
      " 18  cz_timezone         669746 non-null  object \n",
      " 19  end_date_time       669746 non-null  object \n",
      " 20  injuries_direct     669746 non-null  int64  \n",
      " 21  injuries_indirect   669746 non-null  int64  \n",
      " 22  deaths_direct       669746 non-null  int64  \n",
      " 23  deaths_indirect     669746 non-null  int64  \n",
      " 24  damage_property     533328 non-null  object \n",
      " 25  damage_crops        535619 non-null  object \n",
      " 26  source              669746 non-null  object \n",
      " 27  magnitude           348945 non-null  float64\n",
      " 28  magnitude_type      252901 non-null  object \n",
      " 29  flood_cause         72392 non-null   object \n",
      " 30  category            309 non-null     float64\n",
      " 31  tor_f_scale         15192 non-null   object \n",
      " 32  tor_length          15192 non-null   float64\n",
      " 33  tor_width           15192 non-null   float64\n",
      " 34  tor_other_wfo       1988 non-null    object \n",
      " 35  tor_other_cz_state  1988 non-null    object \n",
      " 36  tor_other_cz_fips   1988 non-null    float64\n",
      " 37  tor_other_cz_name   1988 non-null    object \n",
      " 38  begin_range         410429 non-null  float64\n",
      " 39  begin_azimuth       410429 non-null  object \n",
      " 40  begin_location      410429 non-null  object \n",
      " 41  end_range           410429 non-null  float64\n",
      " 42  end_azimuth         410429 non-null  object \n",
      " 43  end_location        410429 non-null  object \n",
      " 44  begin_lat           410429 non-null  float64\n",
      " 45  begin_lon           410429 non-null  float64\n",
      " 46  end_lat             410429 non-null  float64\n",
      " 47  end_lon             410429 non-null  float64\n",
      " 48  episode_narrative   669746 non-null  object \n",
      " 49  event_narrative     529683 non-null  object \n",
      " 50  data_source         669746 non-null  object \n",
      "dtypes: float64(11), int64(15), object(25)\n",
      "memory usage: 260.6+ MB\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:29:01.999998Z",
     "start_time": "2024-09-24T11:29:01.855042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Based on the data dictionary, we can drop the following columns: `category`\n",
    "df_details.drop(columns=['category'], inplace=True)"
   ],
   "id": "81fc75fe442b68da",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:29:09.516936Z",
     "start_time": "2024-09-24T11:29:07.163723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert `begin_date_time` and `end_date_time` columns to datetime type. Current data follows the format 11-DEC-21 03:49:00\n",
    "df_details['begin_date_time'] = pd.to_datetime(df_details['begin_date_time'], format='%d-%b-%y %H:%M:%S')\n",
    "df_details['end_date_time'] = pd.to_datetime(df_details['end_date_time'], format='%d-%b-%y %H:%M:%S')\n",
    "\n",
    "# Convert `tor_other_cz_fips` to integer type\n",
    "df_details['tor_other_cz_fips'] = df_details['tor_other_cz_fips'].astype('Int64')\n",
    "\n",
    "# Convert `state`, `month_name`, `event_type`, `cz_type`, `wfo`, `cz_timezone`, `magnitude_type`, `tof_f_scale`, `tor_other_wfo`, `tor_other_cz_state` to categorical type\n",
    "df_details['state'] = df_details['state'].astype('category')\n",
    "df_details['month_name'] = df_details['month_name'].astype('category')\n",
    "df_details['event_type'] = df_details['event_type'].astype('category')\n",
    "df_details['cz_type'] = df_details['cz_type'].astype('category')\n",
    "df_details['cz_timezone'] = df_details['cz_timezone'].astype('category')\n",
    "df_details['magnitude_type'] = df_details['magnitude_type'].astype('category')\n",
    "df_details['tor_f_scale'] = df_details['tor_f_scale'].astype('category')\n",
    "df_details['tor_other_cz_state'] = df_details['tor_other_cz_state'].astype('category')"
   ],
   "id": "19c6c9a6325401f2",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:29:10.296670Z",
     "start_time": "2024-09-24T11:29:10.060639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the data information\n",
    "df_details.info()"
   ],
   "id": "bf471852c85d51f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 669746 entries, 0 to 669745\n",
      "Data columns (total 50 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   begin_yearmonth     669746 non-null  int64         \n",
      " 1   begin_day           669746 non-null  int64         \n",
      " 2   begin_time          669746 non-null  int64         \n",
      " 3   end_yearmonth       669746 non-null  int64         \n",
      " 4   end_day             669746 non-null  int64         \n",
      " 5   end_time            669746 non-null  int64         \n",
      " 6   episode_id          669746 non-null  int64         \n",
      " 7   event_id            669746 non-null  int64         \n",
      " 8   state               669746 non-null  category      \n",
      " 9   state_fips          669746 non-null  int64         \n",
      " 10  year                669746 non-null  int64         \n",
      " 11  month_name          669746 non-null  category      \n",
      " 12  event_type          669746 non-null  category      \n",
      " 13  cz_type             669746 non-null  category      \n",
      " 14  cz_fips             669746 non-null  int64         \n",
      " 15  cz_name             669746 non-null  object        \n",
      " 16  wfo                 669746 non-null  object        \n",
      " 17  begin_date_time     669746 non-null  datetime64[ns]\n",
      " 18  cz_timezone         669746 non-null  category      \n",
      " 19  end_date_time       669746 non-null  datetime64[ns]\n",
      " 20  injuries_direct     669746 non-null  int64         \n",
      " 21  injuries_indirect   669746 non-null  int64         \n",
      " 22  deaths_direct       669746 non-null  int64         \n",
      " 23  deaths_indirect     669746 non-null  int64         \n",
      " 24  damage_property     533328 non-null  object        \n",
      " 25  damage_crops        535619 non-null  object        \n",
      " 26  source              669746 non-null  object        \n",
      " 27  magnitude           348945 non-null  float64       \n",
      " 28  magnitude_type      252901 non-null  category      \n",
      " 29  flood_cause         72392 non-null   object        \n",
      " 30  tor_f_scale         15192 non-null   category      \n",
      " 31  tor_length          15192 non-null   float64       \n",
      " 32  tor_width           15192 non-null   float64       \n",
      " 33  tor_other_wfo       1988 non-null    object        \n",
      " 34  tor_other_cz_state  1988 non-null    category      \n",
      " 35  tor_other_cz_fips   1988 non-null    Int64         \n",
      " 36  tor_other_cz_name   1988 non-null    object        \n",
      " 37  begin_range         410429 non-null  float64       \n",
      " 38  begin_azimuth       410429 non-null  object        \n",
      " 39  begin_location      410429 non-null  object        \n",
      " 40  end_range           410429 non-null  float64       \n",
      " 41  end_azimuth         410429 non-null  object        \n",
      " 42  end_location        410429 non-null  object        \n",
      " 43  begin_lat           410429 non-null  float64       \n",
      " 44  begin_lon           410429 non-null  float64       \n",
      " 45  end_lat             410429 non-null  float64       \n",
      " 46  end_lon             410429 non-null  float64       \n",
      " 47  episode_narrative   669746 non-null  object        \n",
      " 48  event_narrative     529683 non-null  object        \n",
      " 49  data_source         669746 non-null  object        \n",
      "dtypes: Int64(1), category(8), datetime64[ns](2), float64(9), int64(15), object(15)\n",
      "memory usage: 220.4+ MB\n"
     ]
    }
   ],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:29:14.485443Z",
     "start_time": "2024-09-24T11:29:13.243409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now let check for duplicates in the data\n",
    "duplicates = df_details.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ],
   "id": "b7cd97d663e2622b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's great news. We have no duplicated rows in our data. Let's proceed to check for missing values in the data.",
   "id": "cb3b6ebf75b11f86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:29:16.927438Z",
     "start_time": "2024-09-24T11:29:16.743173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values in the data\n",
    "missing_values = df_details.isnull().sum()\n",
    "print(\"Missing values in the data:\")\n",
    "missing_values[missing_values > 0].sort_values(ascending=False)"
   ],
   "id": "7a7b43bb0017ef34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tor_other_cz_name     667758\n",
       "tor_other_wfo         667758\n",
       "tor_other_cz_state    667758\n",
       "tor_other_cz_fips     667758\n",
       "tor_f_scale           654554\n",
       "tor_length            654554\n",
       "tor_width             654554\n",
       "flood_cause           597354\n",
       "magnitude_type        416845\n",
       "magnitude             320801\n",
       "end_lat               259317\n",
       "end_lon               259317\n",
       "begin_range           259317\n",
       "begin_azimuth         259317\n",
       "begin_location        259317\n",
       "end_range             259317\n",
       "end_azimuth           259317\n",
       "end_location          259317\n",
       "begin_lat             259317\n",
       "begin_lon             259317\n",
       "event_narrative       140063\n",
       "damage_property       136418\n",
       "damage_crops          134127\n",
       "dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have several columns with missing values. Let's check for the columns with missing values.\n",
   "id": "e7287e0d5f06fbc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Start with `tor_f_scale`, `tor_length`, and `tor_width` columns.\n",
    "Based on the Data Dictionary provided at the beginning I assume columns like `tor_f_scale`, `tor_length`, and `tor_width` are relevant only for tornado events. Itâ€™s expected that these fields would be missing for non-tornado events. Let's prove it."
   ],
   "id": "822f3f28b6619814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:54:04.457344Z",
     "start_time": "2024-09-24T11:54:04.156624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select the tornado-specific columns\n",
    "tornado_columns = ['tor_f_scale', 'tor_length', 'tor_width']\n",
    "\n",
    "# Check if any of these columns is populated for `event_type` not containing 'Tornado'\n",
    "non_tornado = df_details[~df_details['event_type'].str.contains('Tornado', na=False)][tornado_columns].notnull().any(axis=1).sum()\n",
    "\n",
    "# Check if these columns are NOT all null for event_type containing 'Tornado'\n",
    "tornado= df_details[df_details['event_type'].str.contains('Tornado', na=False)][tornado_columns].isnull().all(axis=1).sum()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of non-tornado events with tornado-specific columns filled: {non_tornado}\")\n",
    "print(f\"Number of tornado events with missing tornado-specific columns: {tornado}\")"
   ],
   "id": "3bd667faddb1b296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-tornado events with tornado-specific columns filled: 0\n",
      "Number of tornado events with missing tornado-specific columns: 0\n"
     ]
    }
   ],
   "execution_count": 180
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result indicates that there are no missing values in the tornado-specific columns (`tor_f_scale`, `tor_length`, and `tor_width`) for events classified as tornadoes in the dataset. This confirms my assumption that these fields are only populated for tornado events and should not have missing values within that context.\n",
    "\n",
    "Thus, we donâ€™t need to worry about these missing values in the broader dataset, as they are valid and expected for non-tornado events."
   ],
   "id": "20e4411c551b8756"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's move on to `tor_other_cz_name`, `tor_other_wfo`, `tor_other_cz_state`, and `tor_other_cz_fips` columns. Based on the Data Dictionary mentioned above, columns like `tor_other_cz_name`, `tor_other_wfo`, `tor_other_cz_state`, and `tor_other_cz_fips` apply only to tornadoes that cross into other geographical areas. If a tornado travels beyond its initial location, these additional columns should contain data.\n",
   "id": "cc9be814aefd3bec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:55:26.099833Z",
     "start_time": "2024-09-24T11:55:25.915026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the columns related to tornadoes that cross geographical areas\n",
    "tornado_other_location_cols = ['tor_other_cz_name', 'tor_other_wfo', 'tor_other_cz_state', 'tor_other_cz_fips']\n",
    "\n",
    "# 1. Prove there are only two outcomes: either all four are null or all four are not null\n",
    "# Create a mask where either all 4 are null or all 4 are not null\n",
    "all_null_or_not_null = (\n",
    "    (df_details[tornado_other_location_cols].isnull().all(axis=1)) | \n",
    "    (df_details[tornado_other_location_cols].notnull().all(axis=1))\n",
    ")\n",
    "\n",
    "# Check if any rows violate this condition\n",
    "invalid_rows = df_details[~all_null_or_not_null]\n",
    "\n",
    "if invalid_rows.empty:\n",
    "    print(\"There are no cases where only some of the 'tor_other_*' columns are null/non-null. The assumption holds.\")\n",
    "else:\n",
    "    print(\"There are cases where only some of the 'tor_other_*' columns are null/non-null:\")\n",
    "    print(invalid_rows)\n",
    "\n",
    "# 2. Prove that if the 4 columns are null, event_type is not Tornado, and if not null, event_type is Tornado and the original locations differs from the other locations\n",
    "\n",
    "# First check if event_type is not Tornado when all 4 columns are null\n",
    "non_tornado_mismatch = df_details[(df_details[tornado_other_location_cols].notnull().all(axis=1)) & (~df_details['event_type'].str.contains('Tornado', na=False))]\n",
    "\n",
    "# Check if the initial locations are different from the other locations when the 4 columns are not null and event_type is Tornado\n",
    "location_mismatch = df_details[(df_details[tornado_other_location_cols].notnull().all(axis=1)) & \n",
    "                               (df_details['event_type'].str.contains('Tornado', na=False)) &\n",
    "                               (df_details['cz_fips'] == df_details['tor_other_cz_fips']) &\n",
    "                                (df_details['cz_name'] == df_details['tor_other_cz_name']) &\n",
    "                                (df_details['wfo'] == df_details['tor_other_wfo'])]\n",
    "\n",
    "# Output the results\n",
    "if non_tornado_mismatch.empty and location_mismatch.empty:\n",
    "    print(\"All conditions hold: tornado events have 'tor_other_*' columns populated and locations differ when these columns are populated.\")\n",
    "else:\n",
    "    if not non_tornado_mismatch.empty:\n",
    "        print(\"There are non-tornado events where 'tor_other_*' columns are populated:\")\n",
    "        print(non_tornado_mismatch[['event_type'] + tornado_other_location_cols])\n",
    "    if not location_mismatch.empty:\n",
    "        print(\"There are tornado events where the initial location equals the other location but 'tor_other_*' columns are populated:\")\n",
    "        print(location_mismatch[['event_id', 'event_type', 'cz_name', 'wfo', 'cz_type', 'cz_fips',] + tornado_other_location_cols])"
   ],
   "id": "cdf7d03dc4ce187c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no cases where only some of the 'tor_other_*' columns are null/non-null. The assumption holds.\n",
      "All conditions hold: tornado events have 'tor_other_*' columns populated and locations differ when these columns are populated.\n"
     ]
    }
   ],
   "execution_count": 181
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This means that these columns only contain data when a tornado crosses into a new geographical area. If a tornado stays within one boundary, these columns remain null. This is consistent with the data dictionary and the expected behavior of the dataset.\n",
   "id": "9dd9e5604ddd96a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can move on to the other columns related to geographical locations (`end_lat`, `end_lon`, `begin_range`, `begin_azimuth`, `begin_location`, `end_range`, `end_azimuth`, `end_location`, `begin_lat`, `begin_lon`). I noticed that they all have the same number of rows with missing values. So, I suspect either all have values or are all null in a row, with no partial cases where some columns are populated while others are not. Let's verify this assumption.",
   "id": "d125639e8a85d12b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:05:57.310564Z",
     "start_time": "2024-09-24T12:05:57.129540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select the relevant columns\n",
    "geo_columns = ['end_lat', 'end_lon', 'begin_range', 'begin_azimuth', 'begin_location', \n",
    "               'end_range', 'end_azimuth', 'end_location', 'begin_lat', 'begin_lon']\n",
    "\n",
    "# Create a subset of just those columns\n",
    "geo_data = df_details[geo_columns]\n",
    "\n",
    "# Check if all columns are either fully null or fully populated\n",
    "all_null_or_all_filled = geo_data.isnull().all(axis=1) | geo_data.notnull().all(axis=1)\n",
    "\n",
    "# Count the number of rows where some columns are null but others are not\n",
    "partially_null_rows_count = (~all_null_or_all_filled).sum()\n",
    "\n",
    "print(f\"Number of rows where some columns are null and others are filled: {partially_null_rows_count}\")"
   ],
   "id": "5ba93f7d3046e12a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where some columns are null and others are filled: 0\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, that confirms my theory. These columns are either all being populated or all null in a row, with no partial cases where some columns are populated while others are not. This leads to me to believe that these columns are not always required for every event in the dataset. It makes sense that events like droughts or heat waves, for example, may not have clearly defined start or end geographical locations because they can affect broad regions over time rather than specific points.\n",
    "\n",
    "We can check if the missing geographical data is associated with specific types of events that typically donâ€™t have a clear starting or ending location. "
   ],
   "id": "ca3f8ac5e5ee557"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:06:04.991166Z",
     "start_time": "2024-09-24T12:06:04.781398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter rows where the geographical data is missing\n",
    "missing_geo_data = df_details[geo_data.isnull().all(axis=1)]\n",
    "\n",
    "# Count the event types for these rows\n",
    "missing_geo_event_types = missing_geo_data['event_type'].value_counts()\n",
    "\n",
    "print(\"Event types associated with missing geographical data:\")\n",
    "missing_geo_event_types"
   ],
   "id": "4a2be9d54f772b1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event types associated with missing geographical data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Winter Weather                40129\n",
       "High Wind                     38392\n",
       "Winter Storm                  31873\n",
       "Drought                       31702\n",
       "Heavy Snow                    24229\n",
       "Heat                          13209\n",
       "Excessive Heat                11608\n",
       "Strong Wind                   11197\n",
       "Extreme Cold/Wind Chill        9508\n",
       "Dense Fog                      7529\n",
       "Cold/Wind Chill                6295\n",
       "Frost/Freeze                   6087\n",
       "Blizzard                       5888\n",
       "High Surf                      4326\n",
       "Wildfire                       3447\n",
       "Tropical Storm                 2664\n",
       "Ice Storm                      2566\n",
       "Coastal Flood                  2385\n",
       "Lake-Effect Snow               1028\n",
       "Dust Storm                      982\n",
       "Rip Current                     868\n",
       "Marine Tropical Storm           506\n",
       "Storm Surge/Tide                443\n",
       "Astronomical Low Tide           353\n",
       "Avalanche                       348\n",
       "Sleet                           321\n",
       "Lakeshore Flood                 315\n",
       "Hurricane                       274\n",
       "Tropical Depression             200\n",
       "Freezing Fog                    175\n",
       "Dense Smoke                     129\n",
       "Marine Hurricane/Typhoon         85\n",
       "Volcanic Ashfall                 73\n",
       "Sneakerwave                      39\n",
       "Hurricane (Typhoon)              35\n",
       "Marine Tropical Depression       30\n",
       "Seiche                           26\n",
       "Marine Dense Fog                 15\n",
       "Tsunami                          10\n",
       "Tornado                           9\n",
       "Waterspout                        7\n",
       "Thunderstorm Wind                 6\n",
       "Hail                              4\n",
       "Marine Thunderstorm Wind          2\n",
       "Marine Hail                       0\n",
       "Lightning                         0\n",
       "Heavy Rain                        0\n",
       "Marine High Wind                  0\n",
       "Flash Flood                       0\n",
       "Dust Devil                        0\n",
       "Marine Lightning                  0\n",
       "Flood                             0\n",
       "Marine Strong Wind                0\n",
       "Funnel Cloud                      0\n",
       "Debris Flow                       0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This result supports the idea that the missing geographical data (latitude, longitude, range, azimuth, and location) is associated with events that typically donâ€™t have well-defined start and end points.\n",
    "\n",
    "Given this context, we can conclude that the missing values in these columns are valid and do not require further action."
   ],
   "id": "e87c333d7dfd1a8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, let's check the `flood_cause` column. The `flood_cause` column is expected to be populated only for events classified as floods. Let's verify this assumption.",
   "id": "d85f1595a10e9ed8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:12:49.998323Z",
     "start_time": "2024-09-24T12:12:49.763504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if any of the flood events does not have the flood_cause column populated\n",
    "flood = df_details[df_details['event_type'].str.contains('Flood', na=False)]['flood_cause'].isnull().sum()\n",
    "\n",
    "# Check if any of the non-flood events have the flood_cause column populated\n",
    "non_flood = df_details[~df_details['event_type'].str.contains('Flood', na=False)]['flood_cause'].notnull().sum()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of flood events with missing 'flood_cause': {flood}\")\n",
    "print(f\"Number of non-flood events with 'flood_cause' filled: {non_flood}\")"
   ],
   "id": "df667a0415f6cf1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flood events with missing 'flood_cause': 2700\n",
      "Number of non-flood events with 'flood_cause' filled: 1409\n"
     ]
    }
   ],
   "execution_count": 190
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's check what `event_type` values are associated with these exceptions.",
   "id": "578575884e426d71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:20:42.388493Z",
     "start_time": "2024-09-24T12:20:42.273233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the event types for flood events with missing flood_cause\n",
    "flood_missing_cause = df_details[df_details['event_type'].str.contains('Flood', na=False) & df_details['flood_cause'].isnull()]['event_type'].value_counts()\n",
    "\n",
    "# Output the results\n",
    "flood_missing_cause[flood_missing_cause > 0]"
   ],
   "id": "f56d4c72b7865fbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Coastal Flood      2385\n",
       "Lakeshore Flood     315\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T12:20:46.395187Z",
     "start_time": "2024-09-24T12:20:46.361424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the event types for non-flood events with flood_cause filled. Only display rows with count > 0\n",
    "non_flood_cause = df_details[~df_details['event_type'].str.contains('Flood', na=False) & df_details['flood_cause'].notnull()]['event_type'].value_counts()\n",
    "\n",
    "# Output the results\n",
    "non_flood_cause[non_flood_cause > 0]\n"
   ],
   "id": "67c934730924981",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Debris Flow    1409\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 196
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result shows that the `flood_cause` column is filled for events that are not classified as floods. In this case, 1409 rows have the `flood_cause` column populated while their associated `event_type` is \"Debris Flow\".\n",
    "\n",
    "Debris flow can be influenced by several factors, but it typically occurs as a result of **heavy rainfall**. This could explain why the `flood_cause` column is populated for these events. So, we do not need to handle these missing values in the `flood_cause` column as they are valid entries."
   ],
   "id": "e7f252401786f10c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's check the `magnitude` column. The `magnitude` column is expected to be populated for events classified as wind or hail. Let's verify this assumption.",
   "id": "7df80c3afd2f1521"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for missing values in the magnitude column\n",
    "missing_magnitude = df_details['magnitude'].isnull().sum()\n",
    "print(f\"Number of missing values in 'magnitude': {missing_magnitude}\\n\")\n",
    "\n",
    "# Identify event types with missing magnitude\n",
    "missing_magnitude_events = df_details[df_details['magnitude'].isnull()]['event_type'].value_counts()\n",
    "print(\"Event types with missing magnitude:\")\n",
    "missing_magnitude_events"
   ],
   "id": "11a724bf8b773e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check non-null magnitude values and their associated event types\n",
    "non_null_magnitude_events = df_details[df_details['magnitude'].notnull()]['event_type'].value_counts()\n",
    "print(\"Event types with non-null magnitude:\")\n",
    "non_null_magnitude_events"
   ],
   "id": "8b6a44fcd118cb55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The majority of the event types associated with missing `magnitude` values are not typically associated with specific magnitudes, such as \"Flash Flood\", \"Winter Weather\", \"Winter Storm\" and other while the event types that have non-null values for `magnitude` primarily include \"Thunderstorm Wind\", \"Hail\", and \"High Wind\". This aligns with the definition provided in the Data Dictionary. Thus, the missing values in the `magnitude` column are expected and do not require any further action.",
   "id": "1588a6558f15d8d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's check `magnitude_type` column. The `magnitude_type` column is expected to be populated for events classified as wind and has an associated value in the `magnitude` column. Let's verify this assumption.",
   "id": "29524f6c80955208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter rows where magnitude_type is not null\n",
    "magnitude_type_non_null = df_details[df_details['magnitude_type'].notnull()]\n",
    "\n",
    "# Check if magnitude_type rows have non-null values in magnitude (Condition 1)\n",
    "magnitude_missing = magnitude_type_non_null[magnitude_type_non_null['magnitude'].isnull()]\n",
    "\n",
    "# Define wind-related event types (Condition 2)\n",
    "wind_event_types = ['Thunderstorm Wind', 'High Wind', 'Strong Wind', 'Marine Thunderstorm Wind', \n",
    "                    'Marine High Wind', 'Marine Strong Wind']\n",
    "\n",
    "# Check if magnitude_type rows have wind-related event types (Condition 2)\n",
    "non_wind_events = magnitude_type_non_null[~magnitude_type_non_null['event_type'].isin(wind_event_types)]\n",
    "\n",
    "# Output results\n",
    "print(f\"Rows where magnitude_type is non-null but magnitude is null: {len(magnitude_missing)}\")\n",
    "print(f\"Event types with non-null magnitude_type but not wind-related: {len(non_wind_events)}\")\n",
    "\n",
    "# If any violations are found, display the problematic rows\n",
    "if not magnitude_missing.empty:\n",
    "    print(\"Rows with magnitude_type but missing magnitude:\")\n",
    "    print(magnitude_missing[['event_type', 'magnitude_type', 'magnitude']])\n",
    "\n",
    "if not non_wind_events.empty:\n",
    "    print(\"Rows with magnitude_type but non-wind event type:\")\n",
    "    print(non_wind_events[['event_type', 'magnitude_type']])"
   ],
   "id": "9b3da64f01ad93ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `magnitude_type` field is correctly populated only for wind-related events and always has an associated value in the `magnitude` column. No cases were found where `magnitude_type` was non-null without a corresponding value in `magnitude`, or where `magnitude_type` was filled for non-wind events. This confirms that the field is used appropriately in the dataset. So no further action is required.",
   "id": "7b8b4f68baea0a79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's check `event_narrative` column. The `event_narrative` column provides descriptive details of the individual event. Let's verify if the missing values in this column are associated with specific event types.",
   "id": "f0d69ddda7d99f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for missing values in the event_narrative column\n",
    "missing_narrative_events = df_details[df_details['event_narrative'].isnull()]\n",
    "\n",
    "# Count the event types in these rows\n",
    "event_narrative_counts = missing_narrative_events['event_type'].value_counts()\n",
    "\n",
    "print(\"Event types with missing event_narrative:\")\n",
    "event_narrative_counts"
   ],
   "id": "d78b60d8df60a8ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In our analysis, we observed a significant number of missing values in the `event_narrative` column. Given that the narrative primarily provides additional context rather than contributing to the quantitative analysis, we will not be filling in these missing values. Instead, we will retain the existing data as is, ensuring that our analysis remains focused on quantifiable metrics. Furthermore, since the `episode_narrative` column has no missing values, it may serve as a useful supplementary reference, although it will not be included in our main analysis.",
   "id": "14ef129fbf152594"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "For the `damage_property` column, we will impute the missing values, which account for approximately 20.38% of the dataset. Given this high percentage, removing these missing rows could significantly bias our analysis and lead to a loss of valuable information. We will use averages calculated for combinations of `state`, `event_type`, and `year` for most cases, but where all values in a group are missing, we will resort to 0. This mixed approach allows us to consider regional and event-specific variations while still ensuring that we have a method for imputing values when necessary. By leveraging both types of averages, we aim to provide a more accurate estimate than relying solely on an overall average."
   ],
   "id": "972f38dcf74ad43a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert 'damage_property' to numeric\n",
    "def convert_damage(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'K' in value:\n",
    "            return float(value.replace('K', '').strip()) * 1_000\n",
    "        elif 'M' in value:\n",
    "            return float(value.replace('M', '').strip()) * 1_000_000\n",
    "        elif 'B' in value:\n",
    "            return float(value.replace('B', '').strip()) * 1_000_000_000\n",
    "    return np.nan\n",
    "\n",
    "# Apply the conversion\n",
    "df_details['damage_property_numeric'] = df_details['damage_property'].apply(convert_damage)\n",
    "\n",
    "# Calculate average damage_property_numeric by state, event_type, and year\n",
    "averages = df_details.groupby(['state', 'event_type', 'year'])['damage_property_numeric'].mean().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "averages.rename(columns={'damage_property_numeric': 'average_damage_property'}, inplace=True)\n",
    "\n",
    "# Merge the averages back to the original DataFrame\n",
    "df_details = df_details.merge(averages, on=['state', 'event_type', 'year'], how='left')\n",
    "\n",
    "# Identify the groups where all values in a group are missing\n",
    "group_missing = df_details.groupby(['state', 'event_type', 'year'])['damage_property_numeric'].transform(lambda x: x.isna().all())\n",
    "\n",
    "# Impute missing values with the calculated averages\n",
    "df_details['imputed_damage_property'] = df_details['damage_property_numeric'].fillna(df_details['average_damage_property'])\n",
    "\n",
    "# For groups where all values are missing, impute with 0\n",
    "df_details.loc[group_missing, 'imputed_damage_property'] = df_details.loc[group_missing, 'imputed_damage_property'].fillna(0)\n",
    "\n",
    "# Check for remaining missing values\n",
    "remaining_missing = df_details['imputed_damage_property'].isnull().sum()\n",
    "print(f\"Remaining missing values in 'damage_property': {remaining_missing}\")\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_details.drop(columns=['average_damage_property'], inplace=True)"
   ],
   "id": "cced0aa8dcff2126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Similar for `damage_crops` column, we will impute the missing values. We will follow the same approach as we did for the `damage_property` column.",
   "id": "9e5459ff0ca70c5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert 'damage_crops' to numeric\n",
    "df_details['damage_crops_numeric'] = df_details['damage_crops'].apply(convert_damage)\n",
    "\n",
    "# Calculate average damage_crops_numeric by state, event_type, and year\n",
    "averages_crops = df_details.groupby(['state', 'event_type', 'year'])['damage_crops_numeric'].mean().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "averages_crops.rename(columns={'damage_crops_numeric': 'average_damage_crops'}, inplace=True)\n",
    "\n",
    "# Merge the averages back to the original DataFrame\n",
    "df_details = df_details.merge(averages_crops, on=['state', 'event_type', 'year'], how='left')\n",
    "\n",
    "# Identify the groups where all values in a group are missing\n",
    "group_missing_crops = df_details.groupby(['state', 'event_type', 'year'])['damage_crops_numeric'].transform(lambda x: x.isna().all())\n",
    "\n",
    "# Impute missing values with the calculated averages\n",
    "df_details['imputed_damage_crops'] = df_details['damage_crops_numeric'].fillna(df_details['average_damage_crops'])\n",
    "\n",
    "# For groups where all values are missing, impute with 0\n",
    "df_details.loc[group_missing_crops, 'imputed_damage_crops'] = df_details.loc[group_missing_crops, 'imputed_damage_crops'].fillna(0)\n",
    "\n",
    "# Check for remaining missing values\n",
    "remaining_missing_crops = df_details['imputed_damage_crops'].isnull().sum()\n",
    "print(f\"Remaining missing values in 'damage_crops': {remaining_missing_crops}\")\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_details.drop(columns=['average_damage_crops'], inplace=True)"
   ],
   "id": "185eb34b0e5da7b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Appendix",
   "id": "4baff0dca649f4ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate percentage of missing values by state\n",
    "missing_by_state = df_details['damage_property'].isnull().groupby(df_details['state']).mean() * 100\n",
    "\n",
    "missing_by_state.sort_values(ascending=False)"
   ],
   "id": "498c791033c25b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate percentage of missing values by event_type\n",
    "missing_by_event_type = df_details['damage_property'].isnull().groupby(df_details['event_type']).mean() * 100\n",
    "\n",
    "missing_by_event_type.sort_values(ascending=False)"
   ],
   "id": "51e87f585454596c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate percentage of missing values by year\n",
    "missing_by_year = df_details['damage_property'].isnull().groupby(df_details['year']).mean() * 100\n",
    "\n",
    "missing_by_year.sort_values(ascending=False)"
   ],
   "id": "a1508ba1c0f769e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by year, state, and event_type, then calculate the percentage of missing damage_property\n",
    "missing_groups = df_details.groupby(['year', 'state', 'event_type'])['damage_property'].apply(lambda x: x.isna().mean()).reset_index()\n",
    "\n",
    "# Filter for groups where the missing percentage is 100%\n",
    "missing_100_percent = missing_groups[missing_groups['damage_property'] == 1.0]\n",
    "\n",
    "# Display the result\n",
    "missing_100_percent"
   ],
   "id": "b3ba8c071032436a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Outline for Python Notebook: Storm Event Data Analysis\n",
    "\n",
    "#### 1. **Ask**\n",
    "   - **Define the Problem**\n",
    "     - Identify the key questions: \n",
    "       - What types of storms are most common in the US?\n",
    "       - Where do most storms originate?\n",
    "       - How do storms impact communities (injuries, deaths, damage)?\n",
    "   - **Confirm Stakeholder Expectations**\n",
    "     - Define what stakeholders expect from the analysis (e.g., actionable insights, visualizations).\n",
    "\n",
    "#### 2. **Prepare**\n",
    "   - **Collect Data**\n",
    "     - Load the storm event dataset using `pandas`.\n",
    "     - Explore the dataset structure using `info()`, `head()`, and `describe()`.\n",
    "   - **Store Data**\n",
    "     - Save any preliminary data transformations or filtered datasets for later use.\n",
    "\n",
    "#### 3. **Process**\n",
    "   - **Clean the Data**\n",
    "     - Handle missing values and outliers.\n",
    "     - Convert data types as necessary (e.g., date parsing).\n",
    "   - **Transform Data**\n",
    "     - Standardize column names to snake_case.\n",
    "     - Filter data for specific storm events or time periods if needed.\n",
    "     - Create new columns for analysis (e.g., total damage).\n",
    "\n",
    "#### 4. **Analyze**\n",
    "   - **Descriptive Analysis**\n",
    "     - Count unique storm events by type.\n",
    "     - Calculate total injuries, deaths, and property damage.\n",
    "   - **Geospatial Analysis**\n",
    "     - Use latitude and longitude data to plot storm origins on a map.\n",
    "     - Identify regions most affected by specific storm types.\n",
    "   - **Trends and Patterns**\n",
    "     - Analyze trends over time (e.g., increase in storm frequency).\n",
    "\n",
    "#### 5. **Share**\n",
    "   - **Visualizations**\n",
    "     - Create graphs and maps to illustrate key findings.\n",
    "     - Use interactive maps for better engagement.\n",
    "   - **Interpret Results**\n",
    "     - Summarize insights from the data (e.g., most common storms, geographic hotspots).\n",
    "\n",
    "#### 6. **Act**\n",
    "   - **Recommendations**\n",
    "     - Provide actionable insights based on findings.\n",
    "     - Suggest further areas for research or monitoring based on trends.\n",
    "\n",
    "### Notes\n",
    "- Ensure to document each step with comments and markdown cells to explain your thought process and findings.\n",
    "- Include visualizations at appropriate points to enhance understanding.\n"
   ],
   "id": "418a5aaa621d5f32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
