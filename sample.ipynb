{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Case Study: Analyzing U.S. Storm Events for Disaster Preparedness \n",
    "\n",
    "In this case study, we will analyze storm event data to enhance disaster preparedness efforts in the U.S. We will follow a structured data analysis workflow comprising the following phases:\n",
    "\n",
    "1. **Ask**: Define the problem and confirm expectations.\n",
    "2. **Prepare**: Collect and store data for analysis.\n",
    "3. **Process**: Clean and transform data to ensure integrity.\n",
    "4. **Analyze**: Use data analysis tools to draw conclusions.\n",
    "5. **Share**: Interpret and communicate results to make data-driven decisions.\n",
    "6. **Act**: Put insights to work to address the original problem.\n",
    "\n",
    "We will utilize the open-source Storm Event Database provided by the National Oceanic and Atmospheric Administration (NOAA), available at [NOAA Storm Events Database](https://www.ncdc.noaa.gov/stormevents/ftp.jsp). This dataset contains comprehensive information about various storm events across the U.S., including details such as event type, location, and impact."
   ],
   "id": "be7169fdcf7f38c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ask\n",
    "\n",
    "### Problem Statement\n",
    "The U.S. experiences various storm events that can impact public safety, property, and infrastructure. Analyzing the patterns and impacts of these storms will provide insights that can inform disaster preparedness and response strategies.\n",
    "\n",
    "### Key Questions\n",
    "1. **What types of storm events are most prevalent in the U.S.?**  \n",
    "   Identify the most common storm events and their geographical distribution.\n",
    "\n",
    "2. **Where do storm events most frequently occur?**  \n",
    "   Map the geographical distribution of storm events to identify high-risk areas.\n",
    "\n",
    "3. **Are certain regions more prone to specific types of storm events?**  \n",
    "   Analyze the geographic hot spots for storm types to develop tailored regional preparedness plans.\n",
    "\n",
    "4. **What is the temporal distribution of storm events?**  \n",
    "   Analyze trends over time to understand seasonal patterns and changes in frequency.\n",
    "\n",
    "5. **What is the impact of different storm events on injuries, fatalities, and property damage?**  \n",
    "   Assess the severity of various storm types and their associated risks to prioritize high-risk events for preparedness planning.\n",
    "\n",
    "6. **How can this analysis inform disaster preparedness initiatives?**  \n",
    "   Determine actionable insights that can help stakeholders enhance response plans and allocate resources effectively."
   ],
   "id": "2ccab1df027afd0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare\n",
    "\n",
    "### Data Sources\n",
    "We will use the Storm Events Database from the NOAA for the years 2014 to 2024. The dataset can be accessed at [NOAA Storm Events Database](https://www.ncdc.noaa.gov/stormevents/ftp.jsp).\n",
    "\n",
    "### Data Collection\n",
    "The relevant files will be downloaded in CSV.GZ format, which is a compressed version of CSV files.\n",
    "\n",
    "### Data Description\n",
    "The dataset contains 51 columns with various types of information related to storm events. Below is the data dictionary presented in a table format:\n",
    "\n",
    "\n",
    "| Column Name         | Example                                                    | Description                                                                                                             |\n",
    "|:--------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|\n",
    "| begin_yearmonth     | 201212                                                     | The year and month that the event began (YYYYMM format).                                                                |\n",
    "| begin_day           | 31                                                         | The day of the month that the event began (DD format).                                                                  |\n",
    "| begin_time          | 2359                                                       | The time of day that the event began (hhmm format).                                                                     |\n",
    "| end_yearmonth       | 201301                                                     | The year and month that the event ended (YYYYMM format).                                                                |\n",
    "| end_day             | 01                                                         | The day of the month that the event ended (DD format).                                                                  |\n",
    "| end_time            | 0001                                                       | The time of day that the event ended (hhmm format).                                                                     |\n",
    "| episode_id          | 61280                                                      | ID assigned by NWS to denote the storm episode; may contain multiple events.                                            |\n",
    "| event_id            | 383097                                                     | ID assigned by NWS for each individual storm event (Primary database key field).                                        |\n",
    "| state               | GEORGIA                                                    | The state name where the event occurred (ALL CAPS).                                                                     |\n",
    "| state_fips          | 45                                                         | Unique number assigned to the county by NIST (State FIPS).                                                              |\n",
    "| year                | 2000                                                       | The four-digit year for the event in this record.                                                                       |\n",
    "| month_name          | January                                                    | The name of the month for the event (not abbreviated).                                                                  |\n",
    "| event_type          | Hail                                                       | The type of storm event (spelled out; not abbreviated).                                                                 |\n",
    "| cz_type             | C                                                          | Indicates whether the event happened in a County/Parish, NWS Public Forecast Zone, or Marine.                           |\n",
    "| cz_fips             | 245                                                        | The county FIPS number assigned by NIST or NWS Forecast Zone Number.                                                    |\n",
    "| cz_name             | AIKEN                                                      | Name assigned to the county FIPS number or NWS Forecast Zone.                                                           |\n",
    "| wfo                 | CAE                                                        | The NWS Forecast Officeâ€™s area of responsibility in which the event occurred.                                           |\n",
    "| begin_date_time     | 04/1/2012 20:48:00                                         | Start date and time of the event (MM/DD/YYYY hh:mm:ss).                                                                 |\n",
    "| cz_timezone         | EST-5                                                      | Time Zone for the County/Parish, Zone or Marine Name.                                                                   |\n",
    "| end_date_time       | 04/1/2012 21:03:00                                         | End date and time of the event (MM/DD/YYYY hh:mm:ss).                                                                   |\n",
    "| injuries_direct     | 1                                                          | Number of injuries directly caused by the weather event.                                                                |\n",
    "| injuries_indirect   | 0                                                          | Number of injuries indirectly caused by the weather event.                                                              |\n",
    "| deaths_direct       | 0                                                          | Number of deaths directly caused by the weather event.                                                                  |\n",
    "| deaths_indirect     | 0                                                          | Number of deaths indirectly caused by the weather event.                                                                |\n",
    "| damage_property     | 10.00K                                                     | Estimated property damage incurred by the weather event.                                                                |\n",
    "| damage_crops        | 0.00K                                                      | Estimated damage to crops incurred by the weather event.                                                                |\n",
    "| source              | Public                                                     | Source reporting the weather event.                                                                                     |\n",
    "| magnitude           | 0.75                                                       | Measured extent of the magnitude type (only for wind speeds and hail size).                                             |\n",
    "| magnitude_type      | EG                                                         | Type of magnitude measurement (e.g., wind estimated gust).                                                              |\n",
    "| flood_cause         | Ice Jam                                                    | Reported cause of the flood.                                                                                            |\n",
    "| category            |                                                     | Unknown (During the time of downloading this particular file, NCDC has never seen anything provided within this field.) |\n",
    "| tor_f_scale         | EF0                                                        | Enhanced Fujita Scale describing tornado strength.                                                                      |\n",
    "| tor_length          | 0.66                                                       | Length of the tornado while on the ground (in miles).                                                                   |\n",
    "| tor_width           | 25                                                         | Width of the tornado while on the ground (in whole yards).                                                              |\n",
    "| tor_other_wfo       | DDC                                                        | Continuation of a tornado segment as it crossed from one NWS Forecast Office to another.                                |\n",
    "| tor_other_cz_state  | KS                                                         | Two-character representation for the state name of the continuing tornado segment.                                      |\n",
    "| tor_other_cz_fips   | 41                                                         | FIPS number of the county for the continuing tornado segment.                                                           |\n",
    "| tor_other_cz_name   | DICKINSON                                                  | Name of the county for the continuing tornado segment.                                                                  |\n",
    "| begin_range         | 0.59                                                       | Distance to the nearest tenth of a mile to the location referenced.                                                     |\n",
    "| begin_azimuth       | ENE                                                        | 16-point compass direction from the location referenced.                                                                |\n",
    "| begin_location      | PINELAND                                                   | Name of city, town, or village from which the range is calculated.                                                      |\n",
    "| end_range           | 0.66                                                       | See begin_range.                                                                                                        |\n",
    "| end_azimuth         | WNW                                                        | See begin_azimuth.                                                                                                      |\n",
    "| end_location        | RUSK                                                       | See begin_location.                                                                                                     |\n",
    "| begin_lat           | 29.7898                                                    | Latitude in decimal degrees of the begin point of the event.                                                            |\n",
    "| begin_lon           | -98.6406                                                   | Longitude in decimal degrees of the begin point of the event.                                                           |\n",
    "| end_lat             | 29.7158                                                    | Latitude in decimal degrees of the end point of the event.                                                              |\n",
    "| end_lon             | -98.7744                                                   | Longitude in decimal degrees of the end point of the event.                                                             |\n",
    "| episode_narrative   | A strong upper level system over the southern Rockies...   | Narrative depicting the general nature of the episode.                                                                  |\n",
    "| event_narrative     | Heavy rain caused flash flooding across parts of Wilber... | Narrative providing descriptive details of the individual event.                                                        |\n",
    "\n",
    "### Data Storage\n",
    "The data will be stored in a local environment for analysis."
   ],
   "id": "7d62c9500d2cc5bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loading and Initial Exploration",
   "id": "6e241c9556ccc057"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:33:23.087274Z",
     "start_time": "2024-09-23T15:33:23.080667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To begin, let's import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import glob\n",
    "import missingno as msno\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Set display options to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "id": "154448bb887011bb",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T17:22:44.362499Z",
     "start_time": "2024-09-22T17:22:40.000707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In the data folder, we have lots of files. But for this analysis, we only need Storm Details files which have the prefix \"StormEvents_details-ftp_v1.0_d\".\n",
    "# So, let's define the file path pattern\n",
    "file_pattern = \"./data/StormEvents_details-ftp_v1.0_d*.csv.gz\"\n",
    "\n",
    "# Get a list of all files matching the pattern\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "# Read and concatenate all files into a single DataFrame\n",
    "df_list = [pd.read_csv(file, compression='gzip') for file in all_files]\n",
    "df_details = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Based on the data dictionary, we can drop the following columns: `CATEGORY`\n",
    "df_details.drop(columns=['CATEGORY'], inplace=True)\n",
    "\n",
    "# Rename column names to snake case for consistency \n",
    "df_details = df_details.clean_names()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_details.head()"
   ],
   "id": "a8e23b6fa0536bca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   begin_yearmonth  begin_day  begin_time  end_yearmonth  end_day  end_time  \\\n",
       "0           202112         11         349         202112       11       350   \n",
       "1           202112         11         249         202112       11       254   \n",
       "2           202112         11         325         202112       11       327   \n",
       "3           202112         11         232         202112       11       239   \n",
       "4           202112          6         724         202112        6       724   \n",
       "\n",
       "   episode_id  event_id      state  state_fips  ...  end_range end_azimuth  \\\n",
       "0      165322    999750  TENNESSEE          47  ...        3.0          NW   \n",
       "1      165322    999613  TENNESSEE          47  ...        2.0         ESE   \n",
       "2      165322    999636  TENNESSEE          47  ...        2.0          SW   \n",
       "3      165322    999604  TENNESSEE          47  ...        4.0         NNW   \n",
       "4      165321    999306  TENNESSEE          47  ...        1.0           W   \n",
       "\n",
       "  end_location begin_lat  begin_lon  end_lat  end_lon  \\\n",
       "0   HUNTERS PT   36.3178   -86.3235  36.3296 -86.2965   \n",
       "1  BAKERSWORKS   36.0255   -87.3054  36.0736 -87.2330   \n",
       "2        AMQUI   36.2372   -86.7286  36.2572 -86.7035   \n",
       "3     PINEWOOD   35.9205   -87.6423  35.9725 -87.5068   \n",
       "4    JAMESTOWN   36.4322   -84.9405  36.4322 -84.9405   \n",
       "\n",
       "                                   episode_narrative  \\\n",
       "0  One of the worst tornado outbreaks ever record...   \n",
       "1  One of the worst tornado outbreaks ever record...   \n",
       "2  One of the worst tornado outbreaks ever record...   \n",
       "3  One of the worst tornado outbreaks ever record...   \n",
       "4  After some isolated thunderstorms moved across...   \n",
       "\n",
       "                                     event_narrative data_source  \n",
       "0  This small EF-0 tornado was determined through...         CSV  \n",
       "1  This tornado developed just southeast of the D...         CSV  \n",
       "2  Severe straight-line winds caused significant ...         CSV  \n",
       "3  This tornado touched down in far northwest Hic...         CSV  \n",
       "4  A Facebook report indicated trees and power li...         CSV  \n",
       "\n",
       "[5 rows x 50 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin_yearmonth</th>\n",
       "      <th>begin_day</th>\n",
       "      <th>begin_time</th>\n",
       "      <th>end_yearmonth</th>\n",
       "      <th>end_day</th>\n",
       "      <th>end_time</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>state</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>...</th>\n",
       "      <th>end_range</th>\n",
       "      <th>end_azimuth</th>\n",
       "      <th>end_location</th>\n",
       "      <th>begin_lat</th>\n",
       "      <th>begin_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>episode_narrative</th>\n",
       "      <th>event_narrative</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>349</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>350</td>\n",
       "      <td>165322</td>\n",
       "      <td>999750</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>HUNTERS PT</td>\n",
       "      <td>36.3178</td>\n",
       "      <td>-86.3235</td>\n",
       "      <td>36.3296</td>\n",
       "      <td>-86.2965</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>This small EF-0 tornado was determined through...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>249</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>254</td>\n",
       "      <td>165322</td>\n",
       "      <td>999613</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>BAKERSWORKS</td>\n",
       "      <td>36.0255</td>\n",
       "      <td>-87.3054</td>\n",
       "      <td>36.0736</td>\n",
       "      <td>-87.2330</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>This tornado developed just southeast of the D...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>325</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>327</td>\n",
       "      <td>165322</td>\n",
       "      <td>999636</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>AMQUI</td>\n",
       "      <td>36.2372</td>\n",
       "      <td>-86.7286</td>\n",
       "      <td>36.2572</td>\n",
       "      <td>-86.7035</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>Severe straight-line winds caused significant ...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>232</td>\n",
       "      <td>202112</td>\n",
       "      <td>11</td>\n",
       "      <td>239</td>\n",
       "      <td>165322</td>\n",
       "      <td>999604</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>PINEWOOD</td>\n",
       "      <td>35.9205</td>\n",
       "      <td>-87.6423</td>\n",
       "      <td>35.9725</td>\n",
       "      <td>-87.5068</td>\n",
       "      <td>One of the worst tornado outbreaks ever record...</td>\n",
       "      <td>This tornado touched down in far northwest Hic...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202112</td>\n",
       "      <td>6</td>\n",
       "      <td>724</td>\n",
       "      <td>202112</td>\n",
       "      <td>6</td>\n",
       "      <td>724</td>\n",
       "      <td>165321</td>\n",
       "      <td>999306</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>W</td>\n",
       "      <td>JAMESTOWN</td>\n",
       "      <td>36.4322</td>\n",
       "      <td>-84.9405</td>\n",
       "      <td>36.4322</td>\n",
       "      <td>-84.9405</td>\n",
       "      <td>After some isolated thunderstorms moved across...</td>\n",
       "      <td>A Facebook report indicated trees and power li...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:30:46.512021Z",
     "start_time": "2024-09-23T09:30:46.031227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the DataFrame statistics\n",
    "df_details.describe()"
   ],
   "id": "9a9bb92bfbfcefcf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       begin_yearmonth      begin_day     begin_time  end_yearmonth  \\\n",
       "count    669746.000000  669746.000000  669746.000000  669746.000000   \n",
       "mean     201909.724999      14.884967    1262.750553  201909.724999   \n",
       "std         308.331723       9.061630     677.789666     308.331723   \n",
       "min      201401.000000       1.000000       0.000000  201401.000000   \n",
       "25%      201611.000000       7.000000     745.000000  201611.000000   \n",
       "50%      201907.000000      15.000000    1434.000000  201907.000000   \n",
       "75%      202204.000000      23.000000    1800.000000  202204.000000   \n",
       "max      202406.000000      31.000000    2359.000000  202406.000000   \n",
       "\n",
       "             end_day       end_time     episode_id      event_id  \\\n",
       "count  669746.000000  669746.000000  669746.000000  6.697460e+05   \n",
       "mean       16.502822    1454.988749  138990.624232  8.409394e+05   \n",
       "std         9.079046     615.305445   32771.003907  2.038179e+05   \n",
       "min         1.000000       0.000000   80464.000000  4.819030e+05   \n",
       "25%         9.000000    1051.000000  111238.000000  6.643362e+05   \n",
       "50%        16.000000    1600.000000  139511.000000  8.409355e+05   \n",
       "75%        24.000000    1900.000000  167882.000000  1.017476e+06   \n",
       "max        31.000000    2359.000000  194241.000000  1.201899e+06   \n",
       "\n",
       "          state_fips           year  ...      magnitude   tor_length  \\\n",
       "count  669746.000000  669746.000000  ...  348945.000000  15192.00000   \n",
       "mean       33.071031    2019.038506  ...      37.897867      3.16236   \n",
       "std        19.208950       3.084325  ...      23.715012      4.14433   \n",
       "min         1.000000    2014.000000  ...       0.130000      0.01000   \n",
       "25%        19.000000    2016.000000  ...       1.750000      0.50000   \n",
       "50%        31.000000    2019.000000  ...      50.000000      1.66000   \n",
       "75%        46.000000    2022.000000  ...      52.000000      4.20000   \n",
       "max        99.000000    2024.000000  ...     173.000000     41.88000   \n",
       "\n",
       "          tor_width  tor_other_cz_fips    begin_range      end_range  \\\n",
       "count  15192.000000        1988.000000  410429.000000  410429.000000   \n",
       "mean     196.235153         105.344064       2.440858       2.463415   \n",
       "std      296.644097          86.964479       4.563890       4.577299   \n",
       "min        1.000000           1.000000       0.000000       0.000000   \n",
       "25%       50.000000          45.000000       1.000000       1.000000   \n",
       "50%      100.000000          91.000000       1.000000       1.000000   \n",
       "75%      200.000000         143.000000       3.000000       3.000000   \n",
       "max     3960.000000         820.000000     185.000000     185.000000   \n",
       "\n",
       "           begin_lat      begin_lon        end_lat        end_lon  \n",
       "count  410429.000000  410429.000000  410429.000000  410429.000000  \n",
       "mean       37.648997     -90.233009      37.647509     -90.226771  \n",
       "std         5.177361      11.722470       5.178816      11.720015  \n",
       "min       -14.400000    -171.032700     -14.437500    -170.905900  \n",
       "25%        34.310000     -97.260000      34.310000     -97.260000  \n",
       "50%        38.140000     -89.530000      38.140000     -89.517600  \n",
       "75%        41.220000     -81.460000      41.218300     -81.450000  \n",
       "max        70.375400     151.848400      70.264600     151.858900  \n",
       "\n",
       "[8 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin_yearmonth</th>\n",
       "      <th>begin_day</th>\n",
       "      <th>begin_time</th>\n",
       "      <th>end_yearmonth</th>\n",
       "      <th>end_day</th>\n",
       "      <th>end_time</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>tor_length</th>\n",
       "      <th>tor_width</th>\n",
       "      <th>tor_other_cz_fips</th>\n",
       "      <th>begin_range</th>\n",
       "      <th>end_range</th>\n",
       "      <th>begin_lat</th>\n",
       "      <th>begin_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>669746.000000</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>6.697460e+05</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>669746.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>348945.000000</td>\n",
       "      <td>15192.00000</td>\n",
       "      <td>15192.000000</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>410429.000000</td>\n",
       "      <td>410429.000000</td>\n",
       "      <td>410429.000000</td>\n",
       "      <td>410429.000000</td>\n",
       "      <td>410429.000000</td>\n",
       "      <td>410429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>201909.724999</td>\n",
       "      <td>14.884967</td>\n",
       "      <td>1262.750553</td>\n",
       "      <td>201909.724999</td>\n",
       "      <td>16.502822</td>\n",
       "      <td>1454.988749</td>\n",
       "      <td>138990.624232</td>\n",
       "      <td>8.409394e+05</td>\n",
       "      <td>33.071031</td>\n",
       "      <td>2019.038506</td>\n",
       "      <td>...</td>\n",
       "      <td>37.897867</td>\n",
       "      <td>3.16236</td>\n",
       "      <td>196.235153</td>\n",
       "      <td>105.344064</td>\n",
       "      <td>2.440858</td>\n",
       "      <td>2.463415</td>\n",
       "      <td>37.648997</td>\n",
       "      <td>-90.233009</td>\n",
       "      <td>37.647509</td>\n",
       "      <td>-90.226771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>308.331723</td>\n",
       "      <td>9.061630</td>\n",
       "      <td>677.789666</td>\n",
       "      <td>308.331723</td>\n",
       "      <td>9.079046</td>\n",
       "      <td>615.305445</td>\n",
       "      <td>32771.003907</td>\n",
       "      <td>2.038179e+05</td>\n",
       "      <td>19.208950</td>\n",
       "      <td>3.084325</td>\n",
       "      <td>...</td>\n",
       "      <td>23.715012</td>\n",
       "      <td>4.14433</td>\n",
       "      <td>296.644097</td>\n",
       "      <td>86.964479</td>\n",
       "      <td>4.563890</td>\n",
       "      <td>4.577299</td>\n",
       "      <td>5.177361</td>\n",
       "      <td>11.722470</td>\n",
       "      <td>5.178816</td>\n",
       "      <td>11.720015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>201401.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>201401.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80464.000000</td>\n",
       "      <td>4.819030e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-14.400000</td>\n",
       "      <td>-171.032700</td>\n",
       "      <td>-14.437500</td>\n",
       "      <td>-170.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>201611.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>745.000000</td>\n",
       "      <td>201611.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1051.000000</td>\n",
       "      <td>111238.000000</td>\n",
       "      <td>6.643362e+05</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>34.310000</td>\n",
       "      <td>-97.260000</td>\n",
       "      <td>34.310000</td>\n",
       "      <td>-97.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>201907.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1434.000000</td>\n",
       "      <td>201907.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>139511.000000</td>\n",
       "      <td>8.409355e+05</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.66000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.140000</td>\n",
       "      <td>-89.530000</td>\n",
       "      <td>38.140000</td>\n",
       "      <td>-89.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>202204.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>202204.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>167882.000000</td>\n",
       "      <td>1.017476e+06</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2022.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>4.20000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>41.220000</td>\n",
       "      <td>-81.460000</td>\n",
       "      <td>41.218300</td>\n",
       "      <td>-81.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>202406.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>202406.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>194241.000000</td>\n",
       "      <td>1.201899e+06</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>41.88000</td>\n",
       "      <td>3960.000000</td>\n",
       "      <td>820.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>70.375400</td>\n",
       "      <td>151.848400</td>\n",
       "      <td>70.264600</td>\n",
       "      <td>151.858900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check for Data Quality Issues",
   "id": "f3aa5639844431dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T17:23:15.045540Z",
     "start_time": "2024-09-22T17:23:13.471469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now let check for duplicates in the data\n",
    "duplicates = df_details.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ],
   "id": "b7cd97d663e2622b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's great news. We have no duplicated rows in our data. Let's proceed to check for missing values in the data.",
   "id": "cb3b6ebf75b11f86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T17:23:19.331695Z",
     "start_time": "2024-09-22T17:23:19.037664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the DataFrame information\n",
    "df_details.info()"
   ],
   "id": "904c7ad8e3832309",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 669746 entries, 0 to 669745\n",
      "Data columns (total 50 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   begin_yearmonth     669746 non-null  int64  \n",
      " 1   begin_day           669746 non-null  int64  \n",
      " 2   begin_time          669746 non-null  int64  \n",
      " 3   end_yearmonth       669746 non-null  int64  \n",
      " 4   end_day             669746 non-null  int64  \n",
      " 5   end_time            669746 non-null  int64  \n",
      " 6   episode_id          669746 non-null  int64  \n",
      " 7   event_id            669746 non-null  int64  \n",
      " 8   state               669746 non-null  object \n",
      " 9   state_fips          669746 non-null  int64  \n",
      " 10  year                669746 non-null  int64  \n",
      " 11  month_name          669746 non-null  object \n",
      " 12  event_type          669746 non-null  object \n",
      " 13  cz_type             669746 non-null  object \n",
      " 14  cz_fips             669746 non-null  int64  \n",
      " 15  cz_name             669746 non-null  object \n",
      " 16  wfo                 669746 non-null  object \n",
      " 17  begin_date_time     669746 non-null  object \n",
      " 18  cz_timezone         669746 non-null  object \n",
      " 19  end_date_time       669746 non-null  object \n",
      " 20  injuries_direct     669746 non-null  int64  \n",
      " 21  injuries_indirect   669746 non-null  int64  \n",
      " 22  deaths_direct       669746 non-null  int64  \n",
      " 23  deaths_indirect     669746 non-null  int64  \n",
      " 24  damage_property     533328 non-null  object \n",
      " 25  damage_crops        535619 non-null  object \n",
      " 26  source              669746 non-null  object \n",
      " 27  magnitude           348945 non-null  float64\n",
      " 28  magnitude_type      252901 non-null  object \n",
      " 29  flood_cause         72392 non-null   object \n",
      " 30  tor_f_scale         15192 non-null   object \n",
      " 31  tor_length          15192 non-null   float64\n",
      " 32  tor_width           15192 non-null   float64\n",
      " 33  tor_other_wfo       1988 non-null    object \n",
      " 34  tor_other_cz_state  1988 non-null    object \n",
      " 35  tor_other_cz_fips   1988 non-null    float64\n",
      " 36  tor_other_cz_name   1988 non-null    object \n",
      " 37  begin_range         410429 non-null  float64\n",
      " 38  begin_azimuth       410429 non-null  object \n",
      " 39  begin_location      410429 non-null  object \n",
      " 40  end_range           410429 non-null  float64\n",
      " 41  end_azimuth         410429 non-null  object \n",
      " 42  end_location        410429 non-null  object \n",
      " 43  begin_lat           410429 non-null  float64\n",
      " 44  begin_lon           410429 non-null  float64\n",
      " 45  end_lat             410429 non-null  float64\n",
      " 46  end_lon             410429 non-null  float64\n",
      " 47  episode_narrative   669746 non-null  object \n",
      " 48  event_narrative     529683 non-null  object \n",
      " 49  data_source         669746 non-null  object \n",
      "dtypes: float64(10), int64(15), object(25)\n",
      "memory usage: 255.5+ MB\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have seneval columns with missing values. Let's check for the columns with missing values.",
   "id": "c0ebde14d7e7caa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:18:53.620018Z",
     "start_time": "2024-09-23T14:18:52.927738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values in the data\n",
    "missing_values = df_details.isnull().sum()\n",
    "print(\"Missing values in the data:\")\n",
    "missing_values[missing_values > 0].sort_values(ascending=False)"
   ],
   "id": "7a7b43bb0017ef34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tor_other_cz_name     667758\n",
       "tor_other_wfo         667758\n",
       "tor_other_cz_state    667758\n",
       "tor_other_cz_fips     667758\n",
       "tor_f_scale           654554\n",
       "tor_length            654554\n",
       "tor_width             654554\n",
       "flood_cause           597354\n",
       "magnitude_type        416845\n",
       "magnitude             320801\n",
       "end_lat               259317\n",
       "end_lon               259317\n",
       "begin_range           259317\n",
       "begin_azimuth         259317\n",
       "begin_location        259317\n",
       "end_range             259317\n",
       "end_azimuth           259317\n",
       "end_location          259317\n",
       "begin_lat             259317\n",
       "begin_lon             259317\n",
       "event_narrative       140063\n",
       "damage_property       136418\n",
       "damage_crops          134127\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Start with `tor_f_scale`, `tor_length`, and `tor_width` columns.\n",
    "Based on the Data Dictionary provided at the beginning I assume columns like `tor_f_scale`, `tor_length`, and `tor_width` are relevant only for tornado events. Itâ€™s expected that these fields would be missing for non-tornado events. Let's prove it."
   ],
   "id": "822f3f28b6619814"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can move on to the other columns related to geographical locations (`end_lat`, `end_lon`, `begin_range`, `begin_azimuth`, `begin_location`, `end_range`, `end_azimuth`, `end_location`, `begin_lat`, `begin_lon`). I noticed that they all have the same number of rows with missing values. So, I suspect either all have values or are all null in a row, with no partial cases where some columns are populated while others are not. Let's verify this assumption.",
   "id": "d125639e8a85d12b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:59:47.039436Z",
     "start_time": "2024-09-23T09:59:46.852945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select the relevant columns\n",
    "geo_columns = ['end_lat', 'end_lon', 'begin_range', 'begin_azimuth', 'begin_location', \n",
    "               'end_range', 'end_azimuth', 'end_location', 'begin_lat', 'begin_lon']\n",
    "\n",
    "# Create a subset of just those columns\n",
    "geo_data = df_details[geo_columns]\n",
    "\n",
    "# Check if all columns are either fully null or fully populated\n",
    "all_null_or_all_filled = geo_data.isnull().all(axis=1) | geo_data.notnull().all(axis=1)\n",
    "\n",
    "# Count the number of rows where some columns are null but others are not\n",
    "partially_null_rows_count = (~all_null_or_all_filled).sum()\n",
    "\n",
    "print(f\"Number of rows where some columns are null and others are filled: {partially_null_rows_count}\")"
   ],
   "id": "5ba93f7d3046e12a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where some columns are null and others are filled: 0\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result indicates that there are no missing values in the tornado-specific columns (`tor_f_scale`, `tor_length`, and `tor_width`) for events classified as tornadoes in the dataset. This confirms my assumption that these fields are only populated for tornado events and should not have missing values within that context.\n",
    "\n",
    "Thus, we donâ€™t need to worry about these missing values in the broader dataset, as they are valid and expected for non-tornado events. "
   ],
   "id": "689d8e80eeda14b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's move on to `tor_other_cz_name`, `tor_other_wfo`, `tor_other_cz_state`, and `tor_other_cz_fips` columns. Based on the Data Dictionary mentioned above, columns like `tor_other_cz_name`, `tor_other_wfo`, `tor_other_cz_state`, and `tor_other_cz_fips` apply only to tornadoes that cross into other geographical areas. If a tornado travels beyond its initial location, these additional columns should contain data.",
   "id": "53b16413564fb969"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:09:44.759057Z",
     "start_time": "2024-09-23T06:09:44.379193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the columns related to tornadoes that cross geographical areas\n",
    "tornado_other_location_cols = ['tor_other_cz_name', 'tor_other_wfo', 'tor_other_cz_state', 'tor_other_cz_fips']\n",
    "\n",
    "# 1. Prove there are only two outcomes: either all four are null or all four are not null\n",
    "# Create a mask where either all 4 are null or all 4 are not null\n",
    "all_null_or_not_null = (\n",
    "    (df_details[tornado_other_location_cols].isnull().all(axis=1)) | \n",
    "    (df_details[tornado_other_location_cols].notnull().all(axis=1))\n",
    ")\n",
    "\n",
    "# Check if any rows violate this condition\n",
    "invalid_rows = df_details[~all_null_or_not_null]\n",
    "\n",
    "if invalid_rows.empty:\n",
    "    print(\"There are no cases where only some of the 'tor_other_*' columns are null/non-null. The assumption holds.\")\n",
    "else:\n",
    "    print(\"There are cases where only some of the 'tor_other_*' columns are null/non-null:\")\n",
    "    print(invalid_rows)\n",
    "\n",
    "# 2. Prove that if the 4 columns are null, event_type is not Tornado, and if not null, event_type is Tornado and the original locations differs from the other locations\n",
    "\n",
    "# First check if event_type is not Tornado when all 4 columns are null\n",
    "non_tornado_mismatch = df_details[(df_details[tornado_other_location_cols].notnull().all(axis=1)) & (~df_details['event_type'].str.contains('Tornado', na=False))]\n",
    "\n",
    "# Check if the initial locations are different from the other locations when the 4 columns are not null and event_type is Tornado\n",
    "location_mismatch = df_details[(df_details[tornado_other_location_cols].notnull().all(axis=1)) & \n",
    "                               (df_details['event_type'].str.contains('Tornado', na=False)) &\n",
    "                               (df_details['cz_fips'] == df_details['tor_other_cz_fips']) &\n",
    "                                (df_details['cz_name'] == df_details['tor_other_cz_name']) &\n",
    "                                (df_details['wfo'] == df_details['tor_other_wfo'])]\n",
    "\n",
    "# Output the results\n",
    "if non_tornado_mismatch.empty and location_mismatch.empty:\n",
    "    print(\"All conditions hold: tornado events have 'tor_other_*' columns populated and locations differ when these columns are populated.\")\n",
    "else:\n",
    "    if not non_tornado_mismatch.empty:\n",
    "        print(\"There are non-tornado events where 'tor_other_*' columns are populated:\")\n",
    "        print(non_tornado_mismatch[['event_type'] + tornado_other_location_cols])\n",
    "    if not location_mismatch.empty:\n",
    "        print(\"There are tornado events where the initial location equals the other location but 'tor_other_*' columns are populated:\")\n",
    "        print(location_mismatch[['event_id', 'event_type', 'cz_name', 'wfo', 'cz_type', 'cz_fips',] + tornado_other_location_cols])"
   ],
   "id": "ecf2a7b63315ae6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no cases where only some of the 'tor_other_*' columns are null/non-null. The assumption holds.\n",
      "All conditions hold: tornado events have 'tor_other_*' columns populated and locations differ when these columns are populated.\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This means that these columns only contain data when a tornado crosses into a new geographical area. If a tornado stays within one boundary, these columns remain null. This is consistent with the data dictionary and the expected behavior of the dataset.",
   "id": "2056f20dcf25a253"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, let's check the `flood_cause` column. The `flood_cause` column is expected to be populated only for events classified as floods. Let's verify this assumption.",
   "id": "d85f1595a10e9ed8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:33:27.816965Z",
     "start_time": "2024-09-23T06:33:27.244436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flood-specific check\n",
    "flood_events = df_details[df_details['event_type'].str.contains('Flood', na=False)]\n",
    "non_flood_events = df_details[~df_details['event_type'].str.contains('Flood', na=False)]\n",
    "print(f\"Non-flood events with flood_cause filled: {non_flood_events['flood_cause'].notnull().sum()}\")"
   ],
   "id": "df667a0415f6cf1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-flood events with flood_cause filled: 1409\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Most of the flood events will have the `flood_cause` populated. Let's check what `event_type` values are associated with the 1409 exceptions above.",
   "id": "578575884e426d71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T07:57:14.265810Z",
     "start_time": "2024-09-23T07:57:14.233040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the event types for non-flood events with flood_cause filled\n",
    "non_flood_events_with_flood_cause = non_flood_events[non_flood_events['flood_cause'].notnull()]\n",
    "\n",
    "non_flood_events_with_flood_cause['event_type'].value_counts()"
   ],
   "id": "f56d4c72b7865fbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Debris Flow    1409\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result shows that the `flood_cause` column is filled for events that are not classified as floods. In this case, 1409 rows have the `flood_cause` column populated while their associated `event_type` is \"Debris Flow\".\n",
    "\n",
    "Debris flow can be influenced by several factors, but it typically occurs as a result of **heavy rainfall**. This could explain why the `flood_cause` column is populated for these events. So, we do not need to handle these missing values in the `flood_cause` column as they are valid entries."
   ],
   "id": "e7f252401786f10c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's check the `magnitude` column. The `magnitude` column is expected to be populated for events classified as wind or hail. Let's verify this assumption.",
   "id": "7df80c3afd2f1521"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:20:22.459904Z",
     "start_time": "2024-09-23T14:20:22.341941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values in the magnitude column\n",
    "missing_magnitude = df_details['magnitude'].isnull().sum()\n",
    "print(f\"Number of missing values in 'magnitude': {missing_magnitude}\\n\")\n",
    "\n",
    "# Identify event types with missing magnitude\n",
    "missing_magnitude_events = df_details[df_details['magnitude'].isnull()]['event_type'].value_counts()\n",
    "print(\"Event types with missing magnitude:\")\n",
    "missing_magnitude_events"
   ],
   "id": "11a724bf8b773e60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in 'magnitude': 320801\n",
      "\n",
      "Event types with missing magnitude:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Flash Flood                   41325\n",
       "Winter Weather                40129\n",
       "Winter Storm                  31873\n",
       "Drought                       31702\n",
       "Flood                         29658\n",
       "Heavy Snow                    24229\n",
       "Tornado                       15192\n",
       "Heavy Rain                    14871\n",
       "Heat                          13209\n",
       "Excessive Heat                11608\n",
       "Extreme Cold/Wind Chill        9508\n",
       "Dense Fog                      7529\n",
       "Cold/Wind Chill                6295\n",
       "Frost/Freeze                   6087\n",
       "Blizzard                       5888\n",
       "High Surf                      4326\n",
       "Lightning                      3519\n",
       "Wildfire                       3447\n",
       "Funnel Cloud                   2959\n",
       "Tropical Storm                 2664\n",
       "Ice Storm                      2566\n",
       "Coastal Flood                  2385\n",
       "Waterspout                     2088\n",
       "Debris Flow                    1409\n",
       "Lake-Effect Snow               1028\n",
       "Dust Storm                      982\n",
       "Rip Current                     868\n",
       "Marine Tropical Storm           506\n",
       "Storm Surge/Tide                443\n",
       "Astronomical Low Tide           353\n",
       "Avalanche                       348\n",
       "Sleet                           321\n",
       "Lakeshore Flood                 315\n",
       "Hurricane                       274\n",
       "Tropical Depression             200\n",
       "Freezing Fog                    175\n",
       "Dense Smoke                     129\n",
       "Marine Hurricane/Typhoon         85\n",
       "Dust Devil                       78\n",
       "Volcanic Ashfall                 73\n",
       "Sneakerwave                      39\n",
       "Hurricane (Typhoon)              35\n",
       "Marine Tropical Depression       30\n",
       "Seiche                           26\n",
       "Marine Dense Fog                 15\n",
       "Tsunami                          10\n",
       "Marine Lightning                  2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:20:40.624159Z",
     "start_time": "2024-09-23T14:20:40.363545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check non-null magnitude values and their associated event types\n",
    "non_null_magnitude_events = df_details[df_details['magnitude'].notnull()]['event_type'].value_counts()\n",
    "print(\"Event types with non-null magnitude:\")\n",
    "non_null_magnitude_events"
   ],
   "id": "8b6a44fcd118cb55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event types with non-null magnitude:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Thunderstorm Wind           178272\n",
       "Hail                         95763\n",
       "High Wind                    38392\n",
       "Marine Thunderstorm Wind     24362\n",
       "Strong Wind                  11197\n",
       "Marine High Wind               602\n",
       "Marine Hail                    281\n",
       "Marine Strong Wind              76\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The majority of the event types associated with missing `magnitude` values are not typically associated with specific magnitudes, such as \"Flash Flood\", \"Winter Weather\", \"Winter Storm\" and other while the event types that have non-null values for `magnitude` primarily include \"Thunderstorm Wind\", \"Hail\", and \"High Wind\". This aligns with the definition provided in the Data Dictionary. Thus, the missing values in the `magnitude` column are expected and do not require any further action.",
   "id": "1588a6558f15d8d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's check `magnitude_type` column. The `magnitude_type` column is expected to be populated for events classified as wind and has an associated value in the `magnitude` column. Let's verify this assumption.",
   "id": "29524f6c80955208"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:46:58.129891Z",
     "start_time": "2024-09-23T09:46:57.893821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter rows where magnitude_type is not null\n",
    "magnitude_type_non_null = df_details[df_details['magnitude_type'].notnull()]\n",
    "\n",
    "# Check if magnitude_type rows have non-null values in magnitude (Condition 1)\n",
    "magnitude_missing = magnitude_type_non_null[magnitude_type_non_null['magnitude'].isnull()]\n",
    "\n",
    "# Define wind-related event types (Condition 2)\n",
    "wind_event_types = ['Thunderstorm Wind', 'High Wind', 'Strong Wind', 'Marine Thunderstorm Wind', \n",
    "                    'Marine High Wind', 'Marine Strong Wind']\n",
    "\n",
    "# Check if magnitude_type rows have wind-related event types (Condition 2)\n",
    "non_wind_events = magnitude_type_non_null[~magnitude_type_non_null['event_type'].isin(wind_event_types)]\n",
    "\n",
    "# Output results\n",
    "print(f\"Rows where magnitude_type is non-null but magnitude is null: {len(magnitude_missing)}\")\n",
    "print(f\"Event types with non-null magnitude_type but not wind-related: {len(non_wind_events)}\")\n",
    "\n",
    "# If any violations are found, display the problematic rows\n",
    "if not magnitude_missing.empty:\n",
    "    print(\"Rows with magnitude_type but missing magnitude:\")\n",
    "    print(magnitude_missing[['event_type', 'magnitude_type', 'magnitude']])\n",
    "\n",
    "if not non_wind_events.empty:\n",
    "    print(\"Rows with magnitude_type but non-wind event type:\")\n",
    "    print(non_wind_events[['event_type', 'magnitude_type']])"
   ],
   "id": "9b3da64f01ad93ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where magnitude_type is non-null but magnitude is null: 0\n",
      "Event types with non-null magnitude_type but not wind-related: 0\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `magnitude_type` field is correctly populated only for wind-related events and always has an associated value in the `magnitude` column. No cases were found where `magnitude_type` was non-null without a corresponding value in `magnitude`, or where `magnitude_type` was filled for non-wind events. This confirms that the field is used appropriately in the dataset. So no further action is required.",
   "id": "7b8b4f68baea0a79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's check the columns related to geographical locations, such as `end_lat`, `end_lon`, `begin_range`, `begin_azimuth`, `begin_location`, `end_range`, `end_azimuth`, `end_location`, `begin_lat`, and `begin_lon`. I noticed they have the same number of rows with missing values. Let's verify if they are all null or all populated in each row.",
   "id": "6c9229cc6de0b77f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:12:22.827992Z",
     "start_time": "2024-09-23T10:12:22.658940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select the relevant columns\n",
    "geo_columns = ['end_lat', 'end_lon', 'begin_range', 'begin_azimuth', 'begin_location', \n",
    "               'end_range', 'end_azimuth', 'end_location', 'begin_lat', 'begin_lon']\n",
    "\n",
    "# Create a subset of just those columns\n",
    "geo_data = df_details[geo_columns]\n",
    "\n",
    "# Check if all columns are either fully null or fully populated\n",
    "all_null_or_all_filled = geo_data.isnull().all(axis=1) | geo_data.notnull().all(axis=1)\n",
    "\n",
    "# Count the number of rows where some columns are null but others are not\n",
    "partially_null_rows_count = (~all_null_or_all_filled).sum()\n",
    "\n",
    "print(f\"Number of rows where some columns are null and others are filled: {partially_null_rows_count}\")"
   ],
   "id": "1b194225e6bc7bc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where some columns are null and others are filled: 0\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, that confirms my theory. These columns are either all being populated or all null in a row, with no partial cases where some columns are populated while others are not. This leads to me to believe that these columns are not always required for every event in the dataset. It makes sense that events like droughts or heat waves, for example, may not have clearly defined start or end geographical locations because they can affect broad regions over time rather than specific points.\n",
    "\n",
    "We can check if the missing geographical data is associated with specific types of events that typically donâ€™t have a clear starting or ending location. "
   ],
   "id": "9c7f676c609aea26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:21:03.879413Z",
     "start_time": "2024-09-23T14:21:03.581473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter rows where the geographical data is missing\n",
    "missing_geo_data = df_details[geo_data.isnull().all(axis=1)]\n",
    "\n",
    "# Count the event types for these rows\n",
    "missing_geo_event_types = missing_geo_data['event_type'].value_counts()\n",
    "\n",
    "print(\"Event types associated with missing geographical data:\")\n",
    "missing_geo_event_types"
   ],
   "id": "d1106632043c394d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event types associated with missing geographical data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Winter Weather                40129\n",
       "High Wind                     38392\n",
       "Winter Storm                  31873\n",
       "Drought                       31702\n",
       "Heavy Snow                    24229\n",
       "Heat                          13209\n",
       "Excessive Heat                11608\n",
       "Strong Wind                   11197\n",
       "Extreme Cold/Wind Chill        9508\n",
       "Dense Fog                      7529\n",
       "Cold/Wind Chill                6295\n",
       "Frost/Freeze                   6087\n",
       "Blizzard                       5888\n",
       "High Surf                      4326\n",
       "Wildfire                       3447\n",
       "Tropical Storm                 2664\n",
       "Ice Storm                      2566\n",
       "Coastal Flood                  2385\n",
       "Lake-Effect Snow               1028\n",
       "Dust Storm                      982\n",
       "Rip Current                     868\n",
       "Marine Tropical Storm           506\n",
       "Storm Surge/Tide                443\n",
       "Astronomical Low Tide           353\n",
       "Avalanche                       348\n",
       "Sleet                           321\n",
       "Lakeshore Flood                 315\n",
       "Hurricane                       274\n",
       "Tropical Depression             200\n",
       "Freezing Fog                    175\n",
       "Dense Smoke                     129\n",
       "Marine Hurricane/Typhoon         85\n",
       "Volcanic Ashfall                 73\n",
       "Sneakerwave                      39\n",
       "Hurricane (Typhoon)              35\n",
       "Marine Tropical Depression       30\n",
       "Seiche                           26\n",
       "Marine Dense Fog                 15\n",
       "Tsunami                          10\n",
       "Tornado                           9\n",
       "Waterspout                        7\n",
       "Thunderstorm Wind                 6\n",
       "Hail                              4\n",
       "Marine Thunderstorm Wind          2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This result supports the idea that the missing geographical data (latitude, longitude, range, azimuth, and location) is associated with events that typically donâ€™t have well-defined start and end points.\n",
    "\n",
    "Given this context, we can conclude that the missing values in these columns are valid and do not require further action."
   ],
   "id": "ca76c3cab2fb20b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's check `event_narrative` column. The `event_narrative` column provides descriptive details of the individual event. Let's verify if the missing values in this column are associated with specific event types.",
   "id": "f0d69ddda7d99f46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:21:12.140739Z",
     "start_time": "2024-09-23T14:21:12.046307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values in the event_narrative column\n",
    "missing_narrative_events = df_details[df_details['event_narrative'].isnull()]\n",
    "\n",
    "# Count the event types in these rows\n",
    "event_narrative_counts = missing_narrative_events['event_type'].value_counts()\n",
    "\n",
    "print(\"Event types with missing event_narrative:\")\n",
    "event_narrative_counts"
   ],
   "id": "d78b60d8df60a8ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event types with missing event_narrative:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Hail                        50120\n",
       "Winter Weather              10362\n",
       "Drought                     10090\n",
       "Thunderstorm Wind            8795\n",
       "Winter Storm                 7033\n",
       "High Wind                    6845\n",
       "Heat                         5986\n",
       "Excessive Heat               4923\n",
       "Heavy Snow                   4547\n",
       "Strong Wind                  4363\n",
       "High Surf                    3844\n",
       "Extreme Cold/Wind Chill      3549\n",
       "Dense Fog                    3347\n",
       "Cold/Wind Chill              3024\n",
       "Marine Thunderstorm Wind     2499\n",
       "Frost/Freeze                 2373\n",
       "Blizzard                     2097\n",
       "Heavy Rain                   1374\n",
       "Flood                         886\n",
       "Wildfire                      787\n",
       "Lake-Effect Snow              566\n",
       "Ice Storm                     487\n",
       "Funnel Cloud                  470\n",
       "Tropical Storm                363\n",
       "Flash Flood                   244\n",
       "Waterspout                    113\n",
       "Lakeshore Flood               109\n",
       "Coastal Flood                 105\n",
       "Marine Tropical Storm          94\n",
       "Astronomical Low Tide          81\n",
       "Volcanic Ashfall               65\n",
       "Marine Hail                    64\n",
       "Rip Current                    58\n",
       "Storm Surge/Tide               57\n",
       "Dust Storm                     52\n",
       "Dense Smoke                    47\n",
       "Freezing Fog                   45\n",
       "Marine High Wind               36\n",
       "Hurricane                      35\n",
       "Lightning                      34\n",
       "Avalanche                      31\n",
       "Marine Hurricane/Typhoon       24\n",
       "Sleet                          13\n",
       "Hurricane (Typhoon)             8\n",
       "Dust Devil                      5\n",
       "Marine Strong Wind              4\n",
       "Debris Flow                     3\n",
       "Seiche                          3\n",
       "Tropical Depression             3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In our analysis, we observed a significant number of missing values in the `event_narrative` column. Given that the narrative primarily provides additional context rather than contributing to the quantitative analysis, we will not be filling in these missing values. Instead, we will retain the existing data as is, ensuring that our analysis remains focused on quantifiable metrics. Furthermore, since the `episode_narrative` column has no missing values, it may serve as a useful supplementary reference, although it will not be included in our main analysis.",
   "id": "14ef129fbf152594"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "For the `damage_property` column, we will impute the missing values, which account for approximately 20.38% of the dataset. Given this high percentage, removing these missing rows could significantly bias our analysis and lead to a loss of valuable information. We will use averages calculated for combinations of `state`, `event_type`, and `year` for most cases, but where all values in a group are missing, we will resort to the overall average. This mixed approach allows us to consider regional and event-specific variations while still ensuring that we have a method for imputing values when necessary. By leveraging both types of averages, we aim to provide a more accurate estimate than relying solely on an overall average."
   ],
   "id": "972f38dcf74ad43a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:04:38.566654Z",
     "start_time": "2024-09-23T16:04:37.909212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert 'damage_property' to numeric\n",
    "def convert_damage(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'K' in value:\n",
    "            return float(value.replace('K', '').strip()) * 1_000\n",
    "        elif 'M' in value:\n",
    "            return float(value.replace('M', '').strip()) * 1_000_000\n",
    "        elif 'B' in value:\n",
    "            return float(value.replace('B', '').strip()) * 1_000_000_000\n",
    "    return np.nan\n",
    "\n",
    "# Apply the conversion\n",
    "df_details['damage_property_numeric'] = df_details['damage_property'].apply(convert_damage)\n",
    "\n",
    "# Calculate average damage_property_numeric by state, event_type, and year\n",
    "averages = df_details.groupby(['state', 'event_type', 'year'])['damage_property_numeric'].mean().reset_index()\n",
    "averages.rename(columns={'damage_property_numeric': 'average_damage_property'}, inplace=True)\n",
    "\n",
    "# Merge the averages back to the original DataFrame\n",
    "df_details = df_details.merge(averages, on=['state', 'event_type', 'year'], how='left')\n",
    "\n",
    "# Impute missing values with the calculated averages\n",
    "df_details['imputed_damage_property'] = df_details['damage_property_numeric'].fillna(df_details['average_damage_property'])\n",
    "\n",
    "# Fill remaining missing values with the overall average\n",
    "overall_avg_damage = df_details['damage_property_numeric'].mean()\n",
    "df_details['imputed_damage_property'] = df_details['imputed_damage_property'].fillna(overall_avg_damage)\n",
    "\n",
    "# Check for remaining missing values\n",
    "remaining_missing = df_details['imputed_damage_property'].isnull().sum()\n",
    "print(f\"Remaining missing values: {remaining_missing}\")"
   ],
   "id": "cced0aa8dcff2126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:14:50.548065Z",
     "start_time": "2024-09-23T16:14:50.323406Z"
    }
   },
   "cell_type": "code",
   "source": "df_details[df_details['damage_property'].isnull()][['year', 'state', 'event_type', 'damage_property', 'damage_property_numeric', 'imputed_damage_property']]",
   "id": "8abf8e0490075c8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        year           state                event_type damage_property  \\\n",
       "24      2021            IOWA                   Tornado             NaN   \n",
       "92      2021            IOWA                   Tornado             NaN   \n",
       "139     2021       LOUISIANA         Thunderstorm Wind             NaN   \n",
       "146     2021     MISSISSIPPI         Thunderstorm Wind             NaN   \n",
       "147     2021     MISSISSIPPI         Thunderstorm Wind             NaN   \n",
       "...      ...             ...                       ...             ...   \n",
       "669738  2017         FLORIDA          Storm Surge/Tide             NaN   \n",
       "669739  2017         FLORIDA          Storm Surge/Tide             NaN   \n",
       "669741  2017  GULF OF MEXICO  Marine Hurricane/Typhoon             NaN   \n",
       "669742  2017  GULF OF MEXICO  Marine Hurricane/Typhoon             NaN   \n",
       "669743  2017         FLORIDA          Storm Surge/Tide             NaN   \n",
       "\n",
       "        damage_property_numeric  imputed_damage_property  \n",
       "24                          NaN             1.359400e+05  \n",
       "92                          NaN             1.359400e+05  \n",
       "139                         NaN             4.477059e+03  \n",
       "146                         NaN             2.150181e+04  \n",
       "147                         NaN             2.150181e+04  \n",
       "...                         ...                      ...  \n",
       "669738                      NaN             1.557692e+06  \n",
       "669739                      NaN             1.557692e+06  \n",
       "669741                      NaN             0.000000e+00  \n",
       "669742                      NaN             0.000000e+00  \n",
       "669743                      NaN             1.557692e+06  \n",
       "\n",
       "[136418 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>event_type</th>\n",
       "      <th>damage_property</th>\n",
       "      <th>damage_property_numeric</th>\n",
       "      <th>imputed_damage_property</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021</td>\n",
       "      <td>IOWA</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.359400e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2021</td>\n",
       "      <td>IOWA</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.359400e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2021</td>\n",
       "      <td>LOUISIANA</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.477059e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2021</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.150181e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2021</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.150181e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669738</th>\n",
       "      <td>2017</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>Storm Surge/Tide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.557692e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669739</th>\n",
       "      <td>2017</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>Storm Surge/Tide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.557692e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669741</th>\n",
       "      <td>2017</td>\n",
       "      <td>GULF OF MEXICO</td>\n",
       "      <td>Marine Hurricane/Typhoon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669742</th>\n",
       "      <td>2017</td>\n",
       "      <td>GULF OF MEXICO</td>\n",
       "      <td>Marine Hurricane/Typhoon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669743</th>\n",
       "      <td>2017</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>Storm Surge/Tide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.557692e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136418 rows Ã— 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Appendix",
   "id": "4baff0dca649f4ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:42:28.279036Z",
     "start_time": "2024-09-23T15:42:28.214851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate percentage of missing values by state\n",
    "missing_by_state = df_details['damage_property'].isnull().groupby(df_details['state']).mean() * 100\n",
    "\n",
    "missing_by_state.sort_values(ascending=False)"
   ],
   "id": "498c791033c25b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "MAINE                   62.568606\n",
       "ATLANTIC NORTH          60.294638\n",
       "GUAM                    54.128440\n",
       "SOUTH CAROLINA          52.813196\n",
       "DISTRICT OF COLUMBIA    45.622120\n",
       "                          ...    \n",
       "LAKE ST CLAIR            0.719424\n",
       "ARIZONA                  0.402510\n",
       "GULF OF ALASKA           0.000000\n",
       "HAWAII WATERS            0.000000\n",
       "GUAM WATERS              0.000000\n",
       "Name: damage_property, Length: 69, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:39:32.576968Z",
     "start_time": "2024-09-23T15:39:32.518870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate percentage of missing values by event_type\n",
    "missing_by_event_type = df_details['damage_property'].isnull().groupby(df_details['event_type']).mean() * 100\n",
    "\n",
    "missing_by_event_type.sort_values(ascending=False)"
   ],
   "id": "51e87f585454596c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_type\n",
       "Ice Storm                     36.204209\n",
       "Marine Dense Fog              33.333333\n",
       "Tropical Depression           33.000000\n",
       "Hurricane (Typhoon)           31.428571\n",
       "Blizzard                      31.266984\n",
       "Drought                       30.505962\n",
       "Dense Fog                     30.203214\n",
       "Coastal Flood                 29.769392\n",
       "Hail                          29.275399\n",
       "Extreme Cold/Wind Chill       29.196466\n",
       "Marine Thunderstorm Wind      27.858961\n",
       "Cold/Wind Chill               27.418586\n",
       "Winter Weather                26.260310\n",
       "Tropical Storm                24.174174\n",
       "Storm Surge/Tide              22.121896\n",
       "Winter Storm                  22.062561\n",
       "Thunderstorm Wind             21.104829\n",
       "Frost/Freeze                  20.174142\n",
       "Heavy Snow                    20.124644\n",
       "High Wind                     19.590019\n",
       "Heat                          19.191460\n",
       "Marine Hail                   18.149466\n",
       "Sleet                         18.068536\n",
       "Wildfire                      17.986655\n",
       "Dust Storm                    17.820774\n",
       "Tornado                       17.515798\n",
       "Freezing Fog                  17.142857\n",
       "Seiche                        15.384615\n",
       "Excessive Heat                14.533081\n",
       "Waterspout                    13.649425\n",
       "Hurricane                     12.773723\n",
       "Strong Wind                   12.422970\n",
       "Heavy Rain                    11.559411\n",
       "Marine Strong Wind            10.526316\n",
       "Tsunami                       10.000000\n",
       "Rip Current                    9.677419\n",
       "Avalanche                      8.620690\n",
       "Lake-Effect Snow               8.560311\n",
       "Marine Hurricane/Typhoon       8.235294\n",
       "Marine Tropical Storm          8.102767\n",
       "Dust Devil                     7.692308\n",
       "Funnel Cloud                   7.671511\n",
       "Marine High Wind               5.647841\n",
       "Lakeshore Flood                4.761905\n",
       "Astronomical Low Tide          4.532578\n",
       "Marine Tropical Depression     3.333333\n",
       "High Surf                      2.820157\n",
       "Lightning                      2.671213\n",
       "Dense Smoke                    0.775194\n",
       "Sneakerwave                    0.000000\n",
       "Flood                          0.000000\n",
       "Flash Flood                    0.000000\n",
       "Volcanic Ashfall               0.000000\n",
       "Marine Lightning               0.000000\n",
       "Debris Flow                    0.000000\n",
       "Name: damage_property, dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:40:40.136990Z",
     "start_time": "2024-09-23T15:40:40.089497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate percentage of missing values by year\n",
    "missing_by_year = df_details['damage_property'].isnull().groupby(df_details['year']).mean() * 100\n",
    "\n",
    "missing_by_year.sort_values(ascending=False)"
   ],
   "id": "a1508ba1c0f769e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2023    23.618144\n",
       "2022    23.453911\n",
       "2024    21.647346\n",
       "2020    20.989246\n",
       "2016    20.878493\n",
       "2021    20.438515\n",
       "2019    19.585624\n",
       "2018    19.524060\n",
       "2017    18.569500\n",
       "2015    17.393407\n",
       "2014    16.965111\n",
       "Name: damage_property, dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:10:22.764364Z",
     "start_time": "2024-09-23T16:10:22.425167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Group by year, state, and event_type, then calculate the percentage of missing damage_property\n",
    "missing_groups = df_details.groupby(['year', 'state', 'event_type'])['damage_property'].apply(lambda x: x.isna().mean()).reset_index()\n",
    "\n",
    "# Filter for groups where the missing percentage is 100%\n",
    "missing_100_percent = missing_groups[missing_groups['damage_property'] == 1.0]\n",
    "\n",
    "# Display the result\n",
    "missing_100_percent"
   ],
   "id": "b3ba8c071032436a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       year          state             event_type  damage_property\n",
       "10     2014        ALABAMA                  Sleet              1.0\n",
       "26     2014         ALASKA       Storm Surge/Tide              1.0\n",
       "54     2014       ARKANSAS           Frost/Freeze              1.0\n",
       "123    2014    CONNECTICUT        Cold/Wind Chill              1.0\n",
       "163    2014        FLORIDA  Astronomical Low Tide              1.0\n",
       "...     ...            ...                    ...              ...\n",
       "10689  2024        VERMONT             Heavy Snow              1.0\n",
       "10721  2024     WASHINGTON                   Heat              1.0\n",
       "10722  2024     WASHINGTON             Heavy Rain              1.0\n",
       "10745  2024  WEST VIRGINIA               Wildfire              1.0\n",
       "10764  2024        WYOMING               Blizzard              1.0\n",
       "\n",
       "[761 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>event_type</th>\n",
       "      <th>damage_property</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Sleet</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2014</td>\n",
       "      <td>ALASKA</td>\n",
       "      <td>Storm Surge/Tide</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2014</td>\n",
       "      <td>ARKANSAS</td>\n",
       "      <td>Frost/Freeze</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2014</td>\n",
       "      <td>CONNECTICUT</td>\n",
       "      <td>Cold/Wind Chill</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2014</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>Astronomical Low Tide</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689</th>\n",
       "      <td>2024</td>\n",
       "      <td>VERMONT</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10721</th>\n",
       "      <td>2024</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>Heat</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>2024</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>Heavy Rain</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10745</th>\n",
       "      <td>2024</td>\n",
       "      <td>WEST VIRGINIA</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10764</th>\n",
       "      <td>2024</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Blizzard</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Outline for Python Notebook: Storm Event Data Analysis\n",
    "\n",
    "#### 1. **Ask**\n",
    "   - **Define the Problem**\n",
    "     - Identify the key questions: \n",
    "       - What types of storms are most common in the US?\n",
    "       - Where do most storms originate?\n",
    "       - How do storms impact communities (injuries, deaths, damage)?\n",
    "   - **Confirm Stakeholder Expectations**\n",
    "     - Define what stakeholders expect from the analysis (e.g., actionable insights, visualizations).\n",
    "\n",
    "#### 2. **Prepare**\n",
    "   - **Collect Data**\n",
    "     - Load the storm event dataset using `pandas`.\n",
    "     - Explore the dataset structure using `info()`, `head()`, and `describe()`.\n",
    "   - **Store Data**\n",
    "     - Save any preliminary data transformations or filtered datasets for later use.\n",
    "\n",
    "#### 3. **Process**\n",
    "   - **Clean the Data**\n",
    "     - Handle missing values and outliers.\n",
    "     - Convert data types as necessary (e.g., date parsing).\n",
    "   - **Transform Data**\n",
    "     - Standardize column names to snake_case.\n",
    "     - Filter data for specific storm events or time periods if needed.\n",
    "     - Create new columns for analysis (e.g., total damage).\n",
    "\n",
    "#### 4. **Analyze**\n",
    "   - **Descriptive Analysis**\n",
    "     - Count unique storm events by type.\n",
    "     - Calculate total injuries, deaths, and property damage.\n",
    "   - **Geospatial Analysis**\n",
    "     - Use latitude and longitude data to plot storm origins on a map.\n",
    "     - Identify regions most affected by specific storm types.\n",
    "   - **Trends and Patterns**\n",
    "     - Analyze trends over time (e.g., increase in storm frequency).\n",
    "\n",
    "#### 5. **Share**\n",
    "   - **Visualizations**\n",
    "     - Create graphs and maps to illustrate key findings.\n",
    "     - Use interactive maps for better engagement.\n",
    "   - **Interpret Results**\n",
    "     - Summarize insights from the data (e.g., most common storms, geographic hotspots).\n",
    "\n",
    "#### 6. **Act**\n",
    "   - **Recommendations**\n",
    "     - Provide actionable insights based on findings.\n",
    "     - Suggest further areas for research or monitoring based on trends.\n",
    "\n",
    "### Notes\n",
    "- Ensure to document each step with comments and markdown cells to explain your thought process and findings.\n",
    "- Include visualizations at appropriate points to enhance understanding.\n"
   ],
   "id": "418a5aaa621d5f32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
